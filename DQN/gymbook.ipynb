{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "from IPython.display import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import animation\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.9                 # discount rate\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.e_decay = .9995             # exploration rate decay\n",
    "        self.e_min = 0.05                # min exploration rate\n",
    "        self.learning_rate = 0.1         # learning rate for algorithm\n",
    "        self.learning_rate_model = 0.01  # learning rate for model (Adam optimizer)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(20, activation='relu', kernel_initializer='uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate_model))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        return self.act_greedy(state)\n",
    "    \n",
    "    def act_greedy(self, state):\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        X = np.zeros((batch_size, self.state_size))\n",
    "        Y = np.zeros((batch_size, self.action_size))\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = minibatch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "            if done:\n",
    "                target[action] = 0\n",
    "            else:\n",
    "                target[action] = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "#                 target[action] = target[action] + self.learning_rate * \\\n",
    "#                     (reward + self.gamma * np.amax(self.model.predict(next_state)[0] - target[action]))\n",
    "            X[i], Y[i] = state, target\n",
    "        self.model.fit(X, Y, batch_size=batch_size, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.e_min:\n",
    "            self.epsilon *= self.e_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(env):\n",
    "    EPISODES = 4000\n",
    "    \n",
    "    # When agent see done=True we put a penalty, but also, done equals True in the last episode\n",
    "    FRAMES_PER_EPISODE = min(1000, env.spec.max_episode_steps - 1)\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    for e in range(1, EPISODES + 1):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        times = []\n",
    "        for time in range(FRAMES_PER_EPISODE):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done or time == FRAMES_PER_EPISODE - 1:\n",
    "                times.append(time)\n",
    "                if e % (EPISODES / 20) == 0:\n",
    "                    print(\"episode: {}/{}, avg time: {}, exploration rate: {:.2}\"\n",
    "                          .format(e, EPISODES, np.average(times), agent.epsilon))\n",
    "                break\n",
    "        agent.replay(32)\n",
    "        \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, play_count, render):\n",
    "    FRAMES = min(500, env.spec.max_episode_steps)\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    times = []\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(play_count):\n",
    "        state = env.reset()\n",
    "        frames = []\n",
    "        for e in range(FRAMES):\n",
    "            if render:\n",
    "                frames.append(env.render(mode = 'rgb_array'))\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            action = agent.act_greedy(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done or e == FRAMES - 1:\n",
    "                times.append(e)\n",
    "                break\n",
    "\n",
    "        if render:\n",
    "            env.render(close=True)\n",
    "            display_frames_as_gif(frames)\n",
    "            render = False\n",
    "            \n",
    "    return times, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-242dc2ee50c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-25 14:41:19,700] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200/4000, avg time: 19.0, exploration rate: 0.91\n",
      "episode: 400/4000, avg time: 22.0, exploration rate: 0.82\n",
      "episode: 600/4000, avg time: 25.0, exploration rate: 0.74\n",
      "episode: 800/4000, avg time: 35.0, exploration rate: 0.67\n",
      "episode: 1000/4000, avg time: 33.0, exploration rate: 0.61\n",
      "episode: 1200/4000, avg time: 139.0, exploration rate: 0.55\n",
      "episode: 1400/4000, avg time: 158.0, exploration rate: 0.5\n",
      "episode: 1600/4000, avg time: 27.0, exploration rate: 0.45\n",
      "episode: 1800/4000, avg time: 119.0, exploration rate: 0.41\n",
      "episode: 2000/4000, avg time: 25.0, exploration rate: 0.37\n",
      "episode: 2200/4000, avg time: 177.0, exploration rate: 0.33\n",
      "episode: 2400/4000, avg time: 115.0, exploration rate: 0.3\n",
      "episode: 2600/4000, avg time: 198.0, exploration rate: 0.27\n",
      "episode: 2800/4000, avg time: 198.0, exploration rate: 0.25\n",
      "episode: 3000/4000, avg time: 26.0, exploration rate: 0.22\n",
      "episode: 3200/4000, avg time: 97.0, exploration rate: 0.2\n",
      "episode: 3400/4000, avg time: 198.0, exploration rate: 0.18\n",
      "episode: 3600/4000, avg time: 109.0, exploration rate: 0.17\n",
      "episode: 3800/4000, avg time: 102.0, exploration rate: 0.15\n",
      "episode: 4000/4000, avg time: 111.0, exploration rate: 0.14\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1337)\n",
    "agent = train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=100, minmax=(102, 127), mean=112.29000000000001, variance=41.985757575757575, skewness=0.40675582009808675, kurtosis=-0.5143571165535552)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJxAspnDWFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fec63a410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times, rewards = evaluate(env, agent, 100, False)\n",
    "plt.hist(times)\n",
    "print(stats.describe(times))\n",
    "\n",
    "# plt.hist(rewards)\n",
    "# print(stats.describe(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1.94731386,  2.17334672,  0.12232983,  0.01602223]]), 0, 1.0, array([[ 1.9907808 ,  1.97670211,  0.12265027,  0.3446602 ]]), False)\n",
      "[[ 7.11011839  6.8039546 ]]\n"
     ]
    }
   ],
   "source": [
    "print(agent.memory[0])\n",
    "print(agent.model.predict(agent.memory[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
