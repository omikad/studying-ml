{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Zero\n",
    "\n",
    "Main differences from AlphaGo Zero:\n",
    "\n",
    "* AlphaZero predicted value is an expectation of reward (-1, 0, 1). AlphaGo Zero predicted probability to win\n",
    "\n",
    "* AlphaZero maintains a single neural network that is updated continually. AlphaGo Zero self-play games were generated by the best player from all previous iterations (which is replaced if a new model has 55% wins against best model)\n",
    "\n",
    "Paper: [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815v1.pdf)\n",
    "\n",
    "Env based on: https://pettingzoo.farama.org/environments/classic/connect_four/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter, deque\n",
    "import torch\n",
    "import copy\n",
    "import datetime\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import resource\n",
    "\n",
    "import connect_four\n",
    "\n",
    "\n",
    "def show_usage():\n",
    "    usage=resource.getrusage(resource.RUSAGE_SELF)\n",
    "    print(f\"usertime={usage[0]} systime={usage[1]} mem={usage[2]/1024.0} mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySummaryWriter(SummaryWriter):\n",
    "    \"\"\"\n",
    "    Wrapper around tensorboard to add points one by one\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.points_cnt = Counter()\n",
    "\n",
    "    def append_scalar(self, name, value):\n",
    "        step = self.points_cnt[name]\n",
    "        self.points_cnt[name] += 1\n",
    "        self.add_scalar(name, value, step)\n",
    "\n",
    "\n",
    "TENSORBOARD = MySummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "0 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "1 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "0 3 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "0 3 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "1 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "0 3 (1, True, (6, 7, 2), array([1, 1, 1, 1, 0, 1, 1], dtype=int32))\n",
      "....1..\n",
      "....0..\n",
      "...01..\n",
      ".1.000.\n",
      ".010011\n",
      "0010111\n"
     ]
    }
   ],
   "source": [
    "def show():\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    state, mask = env.reset()\n",
    "\n",
    "    for step in range(50):\n",
    "        I = np.where(mask == 1)[0]\n",
    "        action = np.random.choice(I)\n",
    "\n",
    "        state, mask, reward, done = env.step(action)\n",
    "\n",
    "        print(step % 2, action, (reward, done, state.board.shape, mask))\n",
    "\n",
    "        if done:\n",
    "            env.render_ascii()\n",
    "            break\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomA vs RandomB:\n",
      "   RandomA  first turn wins: 283\n",
      "   RandomA second turn wins: 223\n",
      "   RandomB  first turn wins: 277\n",
      "   RandomB second turn wins: 214\n",
      "                      draws: 3\n",
      "Random vs AlwaysLeft:\n",
      "   Random  first turn wins: 112\n",
      "   Random second turn wins: 82\n",
      "   AlwaysLeft  first turn wins: 418\n",
      "   AlwaysLeft second turn wins: 388\n",
      "                      draws: 0\n",
      "Random vs AlwaysRight:\n",
      "   Random  first turn wins: 126\n",
      "   Random second turn wins: 65\n",
      "   AlwaysRight  first turn wins: 435\n",
      "   AlwaysRight second turn wins: 374\n",
      "                      draws: 0\n",
      "AlwaysLeft vs AlwaysRight:\n",
      "   AlwaysLeft  first turn wins: 500\n",
      "   AlwaysLeft second turn wins: 0\n",
      "   AlwaysRight  first turn wins: 500\n",
      "   AlwaysRight second turn wins: 0\n",
      "                      draws: 0\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        action = np.random.choice(I)\n",
    "        return action\n",
    "\n",
    "\n",
    "class AlwaysLeftAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        return I[0]\n",
    "\n",
    "\n",
    "class AlwaysRightAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        return I[-1]\n",
    "\n",
    "\n",
    "def battle(env, agent0, agent1, n_games, n_max_steps):\n",
    "    \"\"\"\n",
    "    Play `n_games` where each game is at most `n_max_steps` turns.\n",
    "    Return counter of game results as array `[\n",
    "        agent0  first turn and agent0 wins,\n",
    "        agent0 second turn and agent0 wins,\n",
    "        agent1  first turn and agent1 wins,\n",
    "        agent1 second turn and agent1 wins,\n",
    "        draws\n",
    "    ]`\n",
    "    \"\"\"\n",
    "\n",
    "    results = [0, 0, 0, 0, 0]\n",
    "    agents = [agent0, agent1]\n",
    "\n",
    "    for game in range(n_games):\n",
    "        state, mask = env.reset()\n",
    "\n",
    "        agent_id_first_turn = game % 2  # Switch sides every other game\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "            curr_player_idx = (agent_id_first_turn + step) % 2\n",
    "\n",
    "            action = agents[curr_player_idx].get_action(state, mask)\n",
    "\n",
    "            state, mask, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                if reward == 1:\n",
    "                    # `curr_player_idx` wins!\n",
    "                    if curr_player_idx == agent_id_first_turn:\n",
    "                        results[curr_player_idx * 2] += 1       # player who moved first - wins\n",
    "\n",
    "                    else:\n",
    "                        results[curr_player_idx * 2 + 1] += 1   # player who moved second - wins\n",
    "\n",
    "                else:   # draw\n",
    "                    results[4] += 1\n",
    "\n",
    "                break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def show_sample_agents_battle():\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    for name0, agent0, name1, agent1 in [\n",
    "        (\"RandomA\", RandomAgent(), \"RandomB\", RandomAgent()),\n",
    "        (\"Random\", RandomAgent(), \"AlwaysLeft\", AlwaysLeftAgent()),\n",
    "        (\"Random\", RandomAgent(), \"AlwaysRight\", AlwaysRightAgent()),\n",
    "        (\"AlwaysLeft\", AlwaysLeftAgent(), \"AlwaysRight\", AlwaysRightAgent()),\n",
    "    ]:\n",
    "        battle_results = battle(env, agent0, agent1, n_games=1000, n_max_steps=50)\n",
    "        print(f\"{name0} vs {name1}:\")\n",
    "        print(f\"   {name0}  first turn wins: {battle_results[0]}\")\n",
    "        print(f\"   {name0} second turn wins: {battle_results[1]}\")\n",
    "        print(f\"   {name1}  first turn wins: {battle_results[2]}\")\n",
    "        print(f\"   {name1} second turn wins: {battle_results[3]}\")\n",
    "        print(f\"                      draws: {battle_results[4]}\")\n",
    "\n",
    "show_sample_agents_battle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Zero plays Connect Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayEpisode:\n",
    "    def __init__(self, first_state):\n",
    "        self.states = [first_state]\n",
    "        self.actions = []\n",
    "        self.terminal_rewards = [0, 0]\n",
    "\n",
    "    def on_action(self, action: int, reward: int, done: bool, next_state: connect_four.HashableState):\n",
    "        if done:\n",
    "            player_idx = self.states[-1].player_idx\n",
    "            self.terminal_rewards[player_idx] = reward\n",
    "            self.terminal_rewards[1 - player_idx] = -reward\n",
    "        self.actions.append(action)\n",
    "        self.states.append(next_state)\n",
    "\n",
    "\n",
    "class ExperienceReplay:\n",
    "    \"\"\"\n",
    "    Store played games history\n",
    "    \"\"\"\n",
    "    def __init__(self, max_episodes):\n",
    "        self.episodes = deque()\n",
    "        self.max_episodes = max_episodes\n",
    "\n",
    "    def on_reset(self, state: connect_four.HashableState):\n",
    "        self.episodes.append(ExperienceReplayEpisode(state))\n",
    "        while len(self.episodes) > self.max_episodes:\n",
    "            self.episodes.popleft()\n",
    "\n",
    "    def on_action(self, action: int, reward: int, done: bool, next_state: connect_four.HashableState):\n",
    "        self.episodes[-1].on_action(action, reward, done, next_state)\n",
    "\n",
    "    def clear(self):\n",
    "        self.episodes.clear()\n",
    "\n",
    "    def yield_training_tuples(self):\n",
    "        for episode in self.episodes:\n",
    "            for i in range(len(episode.actions)):\n",
    "                state = episode.states[i]\n",
    "                action = episode.actions[i]\n",
    "                reward = episode.terminal_rewards[state.player_idx]\n",
    "                yield state, action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZeroModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement AlphaGo Zero model with two heads: for action probabilities and state value\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_common = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),   # (6, 7, 2) -> 84\n",
    "            torch.nn.Linear(84, 42),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(42, 32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.model_action_logits = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 7),\n",
    "        )\n",
    "\n",
    "        self.model_state_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def _rotate_board(self, state: connect_four.HashableState):\n",
    "        \"\"\"\n",
    "        Make current player to have coins at `board[:, :, 0]`, and opponent at `board[:, :, 1]`\n",
    "        \"\"\"\n",
    "        board = state.board\n",
    "        if state.player_idx == 1:\n",
    "            board = np.flip(board, axis=2).copy()\n",
    "        return board\n",
    "\n",
    "    def forward(self, board):\n",
    "        common = self.model_common(board)\n",
    "        action_logits = self.model_action_logits(common)\n",
    "        state_value = self.model_state_value(common)\n",
    "\n",
    "        # action_logits.shape == (B, 7)\n",
    "        # state_value.shape == (B, 1)\n",
    "        return action_logits, state_value\n",
    "\n",
    "    def get_p_v_single_state(self, state: connect_four.HashableState, need_p: bool, need_v: bool):\n",
    "        board = self._rotate_board(state)\n",
    "        board = torch.from_numpy(np.expand_dims(board, axis=0)).float()\n",
    "        common = self.model_common(board)\n",
    "\n",
    "        action_logits, state_value = None, None\n",
    "\n",
    "        if need_p:\n",
    "            action_logits = self.model_action_logits(common)\n",
    "            action_probs = torch.nn.functional.softmax(action_logits, dim=1)\n",
    "            action_probs = action_probs.detach().cpu().numpy()[0, :]\n",
    "\n",
    "        if need_v:\n",
    "            state_value = self.model_state_value(common)\n",
    "            state_value = state_value.detach().cpu().numpy()[0, 0]\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "    def get_rollout_action(self, state: connect_four.HashableState, actions_mask):\n",
    "        action_probs, _ = self.get_p_v_single_state(state, need_p=True, need_v=False)\n",
    "        action_probs[actions_mask == 0] = -np.inf\n",
    "\n",
    "        return np.argmax(action_probs)\n",
    "\n",
    "    def go_train(self, tree, experience_replay, optimizer, batch_size):\n",
    "        self.train()\n",
    "\n",
    "        dataset = []\n",
    "        for state, action, reward in experience_replay.yield_training_tuples():\n",
    "            board = self._rotate_board(state)\n",
    "            cnt_visits = tree.N_cnt_visits[state]\n",
    "            probs = cnt_visits / np.sum(cnt_visits)\n",
    "\n",
    "            dataset.append((board, probs, reward))\n",
    "            dataset.append((board[:, ::-1, :].copy(), probs[::-1], reward))  # Horizontal flip\n",
    "\n",
    "        TENSORBOARD.append_scalar('data len', len(dataset))\n",
    "\n",
    "        np.random.shuffle(dataset)\n",
    "        for bi in range(batch_size, len(dataset) + 1, batch_size):\n",
    "            boards = np.zeros((batch_size, 6, 7, 2))\n",
    "            actual_probs = np.zeros((batch_size, 7))\n",
    "            actual_values = np.zeros((batch_size, 1))\n",
    "\n",
    "            for bj, (board, probs, value) in enumerate(dataset[bi - batch_size : bi]):\n",
    "                boards[bj, ...] = board\n",
    "                actual_probs[bj, :] = probs\n",
    "                actual_values[bj, 0] = value\n",
    "\n",
    "            boards = torch.as_tensor(boards).float()\n",
    "            # print(f\"[go_train] boards {boards.shape} {boards.dtype}\")   # (B, rows, cols, 2)\n",
    "\n",
    "            actual_probs = torch.as_tensor(actual_probs).float()\n",
    "            # print(f\"[go_train] actual_probs {actual_probs.shape} {actual_probs.dtype}\")  # (B, cols)\n",
    "\n",
    "            actual_values = torch.as_tensor(actual_values).float()\n",
    "            # print(f\"[go_train] actual_values {actual_values.shape} {actual_values.dtype}\")  # (B, 1)\n",
    "\n",
    "            pred_action_logits, pred_state_value = self.forward(boards)\n",
    "            # print(f\"[go_train] pred_action_logits {pred_action_logits.shape} {pred_action_logits.dtype}\")  # (B, cols)\n",
    "            # print(f\"[go_train] pred_state_value {pred_state_value.shape} {pred_state_value.dtype}\")  # (B, 1)\n",
    "\n",
    "            states_loss = torch.nn.functional.mse_loss(pred_state_value, actual_values)\n",
    "            action_loss = torch.nn.functional.cross_entropy(pred_action_logits, actual_probs)\n",
    "\n",
    "            TENSORBOARD.append_scalar('states_loss', states_loss.item())\n",
    "            TENSORBOARD.append_scalar('action_loss', action_loss.item())\n",
    "\n",
    "            loss = states_loss + action_loss\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "class AlphaGoZeroAgent:\n",
    "    \"\"\"\n",
    "    Trained agent which can play Connect Four game using `AlphaGoZeroModel` model\n",
    "    \"\"\"\n",
    "    def __init__(self, model: AlphaGoZeroModel, n_evaluate_games: int, n_max_steps: int) -> None:\n",
    "        self.n_evaluate_games = n_evaluate_games\n",
    "        self.n_max_steps = n_max_steps\n",
    "        self.random_agent = RandomAgent()\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state: connect_four.HashableState, actions_mask):\n",
    "        self.model.eval()\n",
    "        return self.model.get_rollout_action(state, actions_mask)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def report_model_performance_against_random(self, env):\n",
    "        \"\"\"\n",
    "        Play against random agent\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        results = battle(env, self, self.random_agent, n_games=self.n_evaluate_games, n_max_steps=self.n_max_steps)\n",
    "\n",
    "        wins = results[0] + results[1]\n",
    "        losses = results[2] + results[3]\n",
    "\n",
    "        print(f\"  AlphaZero total wins {wins}, losses {losses}, draws {results[4]}. Detailed result: {results}\")\n",
    "\n",
    "        TENSORBOARD.append_scalar('wins against random', wins/self.n_evaluate_games)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Tree:\n",
    "    \"\"\"\n",
    "    Tree structure which keeps various statistics needed for MCTS\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # Structure of all dicts: { state -> [ action -> value ] }\n",
    "        self.N_cnt_visits = dict()\n",
    "        self.W_sum_vals = dict()\n",
    "        self.Q_values = dict()\n",
    "        self.P_action_probs = dict()\n",
    "\n",
    "        self.V_state_values = dict()  # { state -> predicted state value v(s) }\n",
    "\n",
    "\n",
    "    def get_action(self, state: connect_four.HashableState):\n",
    "        N_s_a = self.N_cnt_visits[state]\n",
    "        Q_s_a = self.Q_values[state]\n",
    "        P_s_a = self.P_action_probs[state]\n",
    "        U_s_a = 4 * P_s_a * (np.sqrt(np.sum(N_s_a)) + 1e-8) / (1 + N_s_a)\n",
    "        action = np.argmax(Q_s_a + U_s_a)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def add_terminal_node(self, state, terminal_value):\n",
    "        self.V_state_values[state] = terminal_value\n",
    "\n",
    "\n",
    "    def add_node(self, state, action_probs, state_value, actions_mask):\n",
    "        actions_len = len(actions_mask)\n",
    "\n",
    "        n_visits = np.zeros(actions_len)\n",
    "        self.N_cnt_visits[state] = n_visits\n",
    "\n",
    "        w_sum_vals = np.zeros(actions_len)\n",
    "        self.W_sum_vals[state] = w_sum_vals\n",
    "\n",
    "        q_vals = np.zeros(actions_len)\n",
    "        q_vals[actions_mask == 0] = -np.inf\n",
    "        self.Q_values[state] = q_vals\n",
    "\n",
    "        self.P_action_probs[state] = action_probs\n",
    "\n",
    "        self.V_state_values[state] = state_value\n",
    "\n",
    "\n",
    "    def backpropagate(self, sa_array):\n",
    "        # `sa_array`: selected path in tree [state0, action0, state1, action1, ..., stateL],\n",
    "\n",
    "        leaf_state = sa_array[-1]\n",
    "        leaf_player_idx = leaf_state.player_idx\n",
    "        leaf_value = self.V_state_values[leaf_state]\n",
    "\n",
    "        assert len(sa_array) % 2 == 1\n",
    "        for si in range(0, len(sa_array) - 1, 2):\n",
    "            state, action = sa_array[si], sa_array[si + 1]\n",
    "\n",
    "            curr_value = leaf_value if state.player_idx == leaf_player_idx else (-leaf_value)\n",
    "\n",
    "            self.N_cnt_visits[state][action] += 1\n",
    "            self.W_sum_vals[state][action] += curr_value\n",
    "            self.Q_values[state][action] = self.W_sum_vals[state][action] / self.N_cnt_visits[state][action]\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        self.N_cnt_visits.clear()\n",
    "        self.W_sum_vals.clear()\n",
    "        self.Q_values.clear()\n",
    "        self.P_action_probs.clear()\n",
    "        self.V_state_values.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model keeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelKeeper:\n",
    "    \"\"\"\n",
    "    Save/load models from checkpoints\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, best_model=None) -> None:\n",
    "        self.best_model = best_model if best_model is not None else AlphaGoZeroModel()\n",
    "\n",
    "    def clone_best_model(self):\n",
    "        new_model = AlphaGoZeroModel()\n",
    "        new_model.load_state_dict(copy.deepcopy(self.best_model.state_dict()))\n",
    "        return new_model\n",
    "\n",
    "    def save_best_model_checkpoint(self, optimizer: torch.optim.Optimizer, log_prefix):\n",
    "        data = {\n",
    "            'model': self.best_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "        checkpoint_path = f\"checkpoint_{ time.strftime('%Y%m%d-%H%M%S') }.ckpt\"\n",
    "        torch.save(data, checkpoint_path)\n",
    "        print(f\"{log_prefix}. Checkpoint saved to `{checkpoint_path}`\")\n",
    "\n",
    "    def load_from_checkpoint(self, checkpoint_path: str):\n",
    "        data = torch.load(checkpoint_path)\n",
    "        model = AlphaGoZeroModel()\n",
    "        model.load_state_dict(data['model'])\n",
    "        self.best_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZero_Impl:\n",
    "    def __init__(self,\n",
    "        env,\n",
    "        model_keeper: ModelKeeper,\n",
    "        model_agent: AlphaGoZeroAgent,\n",
    "        mem_max_episodes: int,\n",
    "        n_iterations: int,\n",
    "        n_games_per_iteration: int,\n",
    "        evaluate_every_iterations_cnt: int,\n",
    "        simulations_cnt: int,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        batch_size: int,\n",
    "        root_dirichlet_alpha: float,\n",
    "        root_exploration_fraction: float,\n",
    "        num_sampling_moves: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        env: connect four compatible environment\n",
    "        model_keeper: save/load model from checkpoints\n",
    "        model_agent: instance of `AlphaGoZeroAgent` which is used to periodically play game for reporting\n",
    "        mem_max_episodes: how many episodes keep in experience replay\n",
    "        n_iterations: total number of iterations to self-play for learning. One iteration consists of playing `n_games_per_iteration` games and one training pass\n",
    "        n_games_per_iteration: number of games to self-play each iteration\n",
    "        evaluate_every_iterations_cnt: how often to report performance against random agent\n",
    "        simulations_cnt: how many MCTS simulations run in each game\n",
    "        learning_rate: learning rate for model optimizer\n",
    "        weight_decay: weight decay for model training loss\n",
    "        batch_size: batch size for model training\n",
    "        root_dirichlet_alpha: noise power at each root state for selection\n",
    "        root_exploration_fraction: noise fraction added at each root state for selection\n",
    "        num_sampling_moves: number of moves to use soft sample (remaining moves use argmax)\n",
    "        \"\"\"\n",
    "        self.root_env = env\n",
    "        self.model_keeper = model_keeper\n",
    "        self.model_agent = model_agent\n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_games_per_iteration = n_games_per_iteration\n",
    "        self.evaluate_every_iterations_cnt = evaluate_every_iterations_cnt\n",
    "        self.simulations_cnt = simulations_cnt\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.root_dirichlet_alpha = root_dirichlet_alpha\n",
    "        self.root_exploration_fraction = root_exploration_fraction\n",
    "        self.num_sampling_moves = num_sampling_moves\n",
    "        self.tree = MCTS_Tree()\n",
    "        self.experience_replay = ExperienceReplay(max_episodes=mem_max_episodes)\n",
    "\n",
    "    def select(self, env):\n",
    "        tree = self.tree\n",
    "        state, actions_mask = env.last()\n",
    "        sa_array = [state]    # selected path [state0, action0, state1, action1, ..., stateK],\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        while sa_array[-1] in tree.V_state_values:\n",
    "            action = tree.get_action(state)\n",
    "            state, actions_mask, reward, done = env.step(action)\n",
    "\n",
    "            sa_array.append(action)\n",
    "            sa_array.append(state)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Guarantees:\n",
    "        #    `sa_array[-1] not in tree.states_set`\n",
    "        #    `actions_mask, reward, done` tied to `sa_array[-1]`\n",
    "        #    `env` moves to state `sa_array[-1]`\n",
    "        return sa_array, actions_mask, reward, done\n",
    "\n",
    "    def expand(self, model: AlphaGoZeroModel, sa_array, actions_mask):\n",
    "        new_leaf_state = sa_array[-1]\n",
    "\n",
    "        action_probs, state_value = model.get_p_v_single_state(new_leaf_state, need_p=True, need_v=True)\n",
    "        action_probs[actions_mask == 0] = 0\n",
    "        action_probs /= np.sum(action_probs)\n",
    "\n",
    "        # last state becomes a leaf node `S_L`\n",
    "        # ?? Maybe immediately update its statistics after expand: https://ai.stackexchange.com/questions/25451/how-does-alphazeros-mcts-work-when-starting-from-the-root-node\n",
    "        self.tree.add_node(new_leaf_state, action_probs, state_value, actions_mask)\n",
    "\n",
    "    def mcts_step(self, root_env, model: AlphaGoZeroModel):\n",
    "        tree = self.tree\n",
    "\n",
    "        root_state, actions_mask = root_env.last()\n",
    "        if root_state not in tree.V_state_values:\n",
    "            self.expand(model, [root_state], actions_mask)\n",
    "\n",
    "        noise = np.random.gamma(self.root_dirichlet_alpha, 1, len(actions_mask))\n",
    "        frac = self.root_exploration_fraction\n",
    "        tree.P_action_probs[root_state] = tree.P_action_probs[root_state] * (1 - frac) + noise * frac\n",
    "\n",
    "        for si in range(self.simulations_cnt):\n",
    "            env = root_env.copy()\n",
    "\n",
    "            sa_array, actions_mask, reward, done = self.select(env)\n",
    "\n",
    "            if done:\n",
    "                # Here `reward` value belongs to a state previous to `sa_array[-1]`\n",
    "                last_state_reward = -reward\n",
    "                tree.add_terminal_node(sa_array[-1], last_state_reward)\n",
    "            else:\n",
    "                self.expand(model, sa_array, actions_mask)\n",
    "\n",
    "            tree.backpropagate(sa_array)\n",
    "\n",
    "        state, actions_mask = root_env.last()\n",
    "        N_s_a = tree.N_cnt_visits[state]\n",
    "\n",
    "        if np.sum(state.board) <= self.num_sampling_moves:\n",
    "            N_s_a = np.sqrt(N_s_a)\n",
    "            pi_probs = N_s_a / np.sum(N_s_a)\n",
    "            pi_probs[actions_mask == 0] = 0\n",
    "            pi_probs /= np.sum(pi_probs)\n",
    "            return np.random.choice(len(actions_mask), p=pi_probs)\n",
    "\n",
    "        else:\n",
    "            return np.argmax(N_s_a)\n",
    "\n",
    "    def collect_tree_statistics_using_self_play(self, model: AlphaGoZeroModel):\n",
    "        experience_replay = self.experience_replay\n",
    "        root_env = self.root_env\n",
    "\n",
    "        for game_i in range(self.n_games_per_iteration):\n",
    "            state, mask = root_env.reset()\n",
    "            experience_replay.on_reset(state)\n",
    "            while True:\n",
    "                action = self.mcts_step(root_env, model)\n",
    "                state, mask, reward, done = root_env.step(action)\n",
    "                experience_replay.on_action(action, reward, done, state)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def go_train(self):\n",
    "        model_keeper = self.model_keeper\n",
    "        model_agent = self.model_agent\n",
    "        experience_replay = self.experience_replay\n",
    "        model = model_keeper.best_model\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        for iter_i in range(self.n_iterations):\n",
    "            self.tree.clear()\n",
    "            experience_replay.clear()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.collect_tree_statistics_using_self_play(model)\n",
    "\n",
    "            model.train()\n",
    "            model.go_train(self.tree, experience_replay, optimizer, self.batch_size)\n",
    "\n",
    "            if (iter_i + 1) % self.evaluate_every_iterations_cnt == 0 or iter_i == self.n_iterations - 1:\n",
    "                model.eval()\n",
    "                model_keeper.save_best_model_checkpoint(optimizer, log_prefix=f\"[{iter_i}/{self.n_iterations - 1}]\")\n",
    "                with torch.no_grad():\n",
    "                    model_agent.report_model_performance_against_random(self.root_env)\n",
    "\n",
    "def train_alphagozero_connectfour(model_keeper=None):\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    if model_keeper is None:\n",
    "        model_keeper = ModelKeeper()\n",
    "\n",
    "    model_agent = AlphaGoZeroAgent(\n",
    "        model_keeper.best_model,\n",
    "        n_evaluate_games=400,\n",
    "        n_max_steps=50)\n",
    "\n",
    "    impl = AlphaGoZero_Impl(\n",
    "        env=env,\n",
    "        model_keeper=model_keeper,\n",
    "        model_agent=model_agent,\n",
    "        mem_max_episodes=100000,\n",
    "        n_iterations=200,\n",
    "        n_games_per_iteration=10,\n",
    "        evaluate_every_iterations_cnt=10,\n",
    "        simulations_cnt=100,\n",
    "        learning_rate=1e-2,\n",
    "        weight_decay=1e-5,\n",
    "        batch_size=16,\n",
    "        root_dirichlet_alpha=0.3,   # same as in chess\n",
    "        root_exploration_fraction=0.25,\n",
    "        num_sampling_moves=10,\n",
    "    )\n",
    "    impl.go_train()\n",
    "\n",
    "    return {\n",
    "        'env': env,\n",
    "        'model_keeper': model_keeper,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/199]. Checkpoint saved to `checkpoint_20240201-112943.ckpt`\n",
      "  AlphaZero total wins 350, losses 50, draws 0. Detailed result: [186, 164, 36, 14, 0]\n",
      "[19/199]. Checkpoint saved to `checkpoint_20240201-113321.ckpt`\n",
      "  AlphaZero total wins 331, losses 69, draws 0. Detailed result: [172, 159, 41, 28, 0]\n",
      "[29/199]. Checkpoint saved to `checkpoint_20240201-113706.ckpt`\n",
      "  AlphaZero total wins 340, losses 60, draws 0. Detailed result: [176, 164, 36, 24, 0]\n",
      "[39/199]. Checkpoint saved to `checkpoint_20240201-114045.ckpt`\n",
      "  AlphaZero total wins 367, losses 33, draws 0. Detailed result: [188, 179, 21, 12, 0]\n",
      "[49/199]. Checkpoint saved to `checkpoint_20240201-114417.ckpt`\n",
      "  AlphaZero total wins 352, losses 48, draws 0. Detailed result: [184, 168, 32, 16, 0]\n",
      "[59/199]. Checkpoint saved to `checkpoint_20240201-114748.ckpt`\n",
      "  AlphaZero total wins 341, losses 59, draws 0. Detailed result: [177, 164, 36, 23, 0]\n",
      "[69/199]. Checkpoint saved to `checkpoint_20240201-115144.ckpt`\n",
      "  AlphaZero total wins 347, losses 53, draws 0. Detailed result: [180, 167, 33, 20, 0]\n",
      "[79/199]. Checkpoint saved to `checkpoint_20240201-115514.ckpt`\n",
      "  AlphaZero total wins 345, losses 55, draws 0. Detailed result: [176, 169, 31, 24, 0]\n",
      "[89/199]. Checkpoint saved to `checkpoint_20240201-115850.ckpt`\n",
      "  AlphaZero total wins 331, losses 69, draws 0. Detailed result: [176, 155, 45, 24, 0]\n",
      "[99/199]. Checkpoint saved to `checkpoint_20240201-120220.ckpt`\n",
      "  AlphaZero total wins 311, losses 89, draws 0. Detailed result: [166, 145, 55, 34, 0]\n",
      "[109/199]. Checkpoint saved to `checkpoint_20240201-120612.ckpt`\n",
      "  AlphaZero total wins 318, losses 82, draws 0. Detailed result: [166, 152, 48, 34, 0]\n"
     ]
    }
   ],
   "source": [
    "# def load(checkpoint_path):\n",
    "#     agent_evaluator = AgentEvaluator(n_evaluate_games=400, n_max_steps=50)\n",
    "#     agent_evaluator.load_from_checkpoint(checkpoint_path)\n",
    "#     return agent_evaluator\n",
    "\n",
    "# started 200 iters with LR=1e-2\n",
    "trained = train_alphagozero_connectfour()                                # Start new training\n",
    "# trained = train_alphagozero_connectfour(trained['model_keeper'])      # Continue existing training\n",
    "# trained = train_alphagozero_connectfour(load('checkpoint.ckpt'))  # Continue from checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usertime=1841.436973 systime=1.906601 mem=449.10546875 mb\n",
      "[482, 414, 86, 18, 0]\n"
     ]
    }
   ],
   "source": [
    "show_usage()\n",
    "\n",
    "def show_trained_model_against_random(trained):\n",
    "    env = trained['env']\n",
    "    model_keeper = trained['model_keeper']\n",
    "    agent = AlphaGoZeroAgent(\n",
    "        model_keeper.best_model,\n",
    "        n_evaluate_games=400,\n",
    "        n_max_steps=50)\n",
    "\n",
    "    model_keeper.best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(battle(env, agent, RandomAgent(), n_games=1000, n_max_steps=50))\n",
    "\n",
    "show_trained_model_against_random(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained model outperforms random agent:\n",
    "\n",
    "If random agent moves first, trained model wins 415 games, loses 85\n",
    "\n",
    "If trained model moves first it wins 446 games, loses 54"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "Action probs: [0.06610451, 0.13449688, 0.58521396, 0.14987804, 0.064306624], state value 0.35236063599586487\n",
      "\n",
      "AI move 2: actions mask [1, 1, 1, 1, 1], reward 0, done False\n",
      ".....\n",
      ".....\n",
      ".....\n",
      "..0..\n"
     ]
    }
   ],
   "source": [
    "class Player:\n",
    "    def __init__(self, trained):\n",
    "        self.env = trained['env']\n",
    "        self.model = trained['model_keeper'].best_model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def start_play_vs_ai(self, human_idx):\n",
    "        env = self.env\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        state, mask = env.reset()\n",
    "        env.render_ascii()\n",
    "\n",
    "        if human_idx > 0:\n",
    "            self.ai_move()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ai_move(self):\n",
    "        env = self.env\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        state, mask = env.last()\n",
    "        action_probs, state_value = model.get_p_v_single_state(state, need_p=True, need_v=True)\n",
    "        action = np.argmax(action_probs)\n",
    "        print(f\"Action probs: {list(action_probs)}, state value {state_value}\")\n",
    "\n",
    "        state, action_mask, reward, done = env.step(action)\n",
    "        print()\n",
    "        print(f\"AI move {action}: actions mask {list(action_mask)}, reward {reward}, done {done}\")\n",
    "        env.render_ascii()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def human_move(self, action):\n",
    "        env = self.env\n",
    "        model = self.model\n",
    "        model.eval()\n",
    "\n",
    "        state, action_mask, reward, done = env.step(action)\n",
    "\n",
    "        print()\n",
    "        print(f\"Your move {action}: actions mask {list(action_mask)}, reward {reward}, done {done}.\")\n",
    "        env.render_ascii()\n",
    "\n",
    "        if not done:\n",
    "            self.ai_move()\n",
    "\n",
    "player = Player(trained)\n",
    "player.start_play_vs_ai(human_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your move 3: actions mask [1, 1, 1, 1, 1], reward 0, done False.\n",
      ".....\n",
      "...1.\n",
      "...0.\n",
      ".1001\n",
      "Action probs: [0.087805696, 0.19854079, 0.3620931, 0.16862911, 0.18293133], state value 0.35661551356315613\n",
      "\n",
      "AI move 2: actions mask [1, 1, 1, 1, 1], reward 0, done False\n",
      ".....\n",
      "...1.\n",
      "..00.\n",
      ".1001\n"
     ]
    }
   ],
   "source": [
    "player.human_move(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Cpuct: supplementary materials p5\n",
    "# LR schedule: supplementary materials p7\n",
    "# supplementary materials has new tree every MCTS run\n",
    "# resignation threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
