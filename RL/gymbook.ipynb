{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(env, agent, params, frames, width, height, greedy=True):\n",
    "#     img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    state = env_reset(env)\n",
    "    img = plt.imshow(state.reshape(width, height))\n",
    "    frame = 0\n",
    "    for _ in range(frames):\n",
    "#         img.set_data(env.render(mode='rgb_array'))\n",
    "        img.set_data(state.reshape(width, height))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        action = agent.act_greedy(state, frame) if greedy else np.random.randint(0, params.action_size)\n",
    "        state, reward, done, _ = env_step(env, action)\n",
    "        if done:\n",
    "            state = env_reset(env)\n",
    "            frame = 0\n",
    "        else:\n",
    "            frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningParameters:\n",
    "    def __init__(self, env, episodes_count):\n",
    "        state = env_reset(env)\n",
    "\n",
    "        self.state_shape = state.shape\n",
    "        self.state_size = np.prod(self.state_shape)\n",
    "        self.action_size = env.action_space.n\n",
    "        self.episodes_count = episodes_count\n",
    "        self.max_frame_in_episode = env.spec.max_episode_steps\n",
    "        self.max_memory_size = 10000\n",
    "        self.episodes_between_think = 1\n",
    "        \n",
    "        self.gamma = 0.95                # discount rate\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.epsilon_start = self.epsilon\n",
    "        self.epsilon_min = 0.0001        # min exploration rate\n",
    "        self.learning_rate = 0.1         # learning rate for algorithm\n",
    "        self.learning_rate_model = 0.01  # learning rate for model\n",
    "        \n",
    "        print(\"State shape {}, actions {}\".format(self.state_shape, self.action_size))\n",
    "\n",
    "    def decay_exploration_rate(self, episode):\n",
    "        # Linear exploration rate decay (lerp)\n",
    "#         self.epsilon = self.epsilon_start - \\\n",
    "#                       (self.epsilon_start - self.epsilon_min) * (float(frame) / self.frames_count)\n",
    "            \n",
    "        # Exponential rate decay\n",
    "        # y(0) = start\n",
    "        # y(1) = start * x\n",
    "        # y(2) = start * x^2\n",
    "        # y(steps) = start * x^steps = min => x = (min/start) ^ (1/steps)\n",
    "        # y(t) = start * x^t\n",
    "        self.epsilon = self.epsilon_start * \\\n",
    "                       math.pow( math.pow(self.epsilon_min / self.epsilon_start, 1.0 / self.episodes_count), episode )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action is added to input as OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionAsInputAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_len = self.params.state_size + self.params.action_size\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, input_len], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, 1], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([input_len, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, 1]))\n",
    "        b1 = tf.Variable(tf.random_normal([1]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'train_op': train_op,\n",
    "            'init': init\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state, frame))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "\n",
    "        X = np.resize(state, (1, self.params.state_size + self.params.action_size))\n",
    "        X[0, self.params.state_size:] = 0\n",
    "        \n",
    "        rewards = np.zeros((self.params.action_size))\n",
    "        for i in range(self.params.action_size):\n",
    "            X[0, self.params.state_size + i] = 1\n",
    "            rewards[i] = session.run(pred, {x: X})[0]\n",
    "            X[0, self.params.state_size + i] = 0\n",
    "        return np.argmax(rewards)\n",
    "    \n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((cnt, 1))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state, frame = self.memory[i]\n",
    "            inp = np.resize(state, (self.params.state_size + self.params.action_size))\n",
    "            inp[self.params.state_size:] = 0\n",
    "            inp[self.params.state_size + action] = 1\n",
    "            X[i], Y[i] = inp, reward\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Run with `params.episodes_between_think = 1`\n",
    "\n",
    "Karpathy: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "TF interpretation: https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name='Placeholder_x')\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name='Placeholder_y')\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "#         pred = tf.nn.softmax(pred)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.params.learning_rate_model, decay=0.99)\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model)\n",
    "    \n",
    "        gradients = optimizer.compute_gradients(cost, var_list=tf.trainable_variables())\n",
    "        train_op = optimizer.apply_gradients(gradients)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, feed_dict={x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, feed_dict={x: [state]})[0]\n",
    "            target[action] = reward\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        _ = session.run(train_op, {x: X, y: Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "#         train_op = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, {x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, {x: [state]})[0]\n",
    "            target[action] = reward + self.params.gamma * \\\n",
    "                             np.amax(session.run(pred, {x: [next_state]})[0])\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-24 21:47:56,131] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4, Observation space (210, 160, 3), 100800 parameters\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input_pong_v0(I):\n",
    "    I = I[35:195] # crop\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I = I[::2,::2,0] + I[1::2,::2,0] + I[::2,1::2,0] + I[1::2,1::2,0]\n",
    "    I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "    I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def preprocess_input_breakout_v0(I):\n",
    "    I = I[35:195, 10:150]  # crop to (160, 140, 3)\n",
    "    I = (I[:,:,0] + I[:,:,1] + I[:,:,2]) / 3\n",
    "    I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# print(env.spec.max_episode_steps)\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "env.my_preprocess_input = preprocess_input_breakout_v0\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}, Observation space {}, {} parameters'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(210, 160, 3)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(env.spec.max_episode_steps)\n",
    "state = env.reset()\n",
    "env.step(1)\n",
    "# for i in range(10):\n",
    "#     state, reward, _, _ = env.step(1)\n",
    "# state, reward, _, _ = env.step(2)\n",
    "# state, reward, _, _ = env.step(3)\n",
    "# state, reward, _, _ = env.step(3)\n",
    "print(state.shape)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5600,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb26605ba10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAAD8CAYAAACFDhMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb26667d510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.my_preprocess_input(env.reset())\n",
    "print(state.shape)\n",
    "# prepro(state).reshape((80, 80)).shape\n",
    "plt.imshow(state.reshape((80, 70)), cmap='Greys')\n",
    "# plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (5600,), actions 4\n",
      "episode: 25/500, reward 1.4, frames 311, exploration rate: 0.65\n",
      "episode: 50/500, reward 1.3, frames 247, exploration rate: 0.41\n",
      "episode: 75/500, reward 0.7, frames 237, exploration rate: 0.26\n"
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env, episodes_count=500)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 500)\n",
    "\n",
    "# agent = PolicyGradientAgent(params)\n",
    "\n",
    "agent = DqnAgent(params)\n",
    "\n",
    "# agent = ActionAsInputAgent(params)\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "session = tf.Session()\n",
    "session.run(agent.model['init'])\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=False)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAAD8CAYAAACFDhMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYZQaYRilRhhGqRGGUWqEYZQa8X/oq9gdD81swgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fefc84e4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(env, agent, params, 100, width=80, height=70, greedy=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
