{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(env, agent, params, frames):\n",
    "#     img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    state = env_reset(env)\n",
    "    img = plt.imshow(state.reshape(80, 80))   # Pong\n",
    "    frame = 0\n",
    "    for _ in range(frames):\n",
    "#         img.set_data(env.render(mode='rgb_array'))\n",
    "        img.set_data(state.reshape(80, 80))   # Pong\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        action = agent.act_greedy(state, frame)\n",
    "        state, reward, done, _ = env_step(env, action)\n",
    "        if done:\n",
    "            state = env_reset(env)\n",
    "            frame = 0\n",
    "        else:\n",
    "            frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningParameters:\n",
    "    def __init__(self, env, episodes_count):\n",
    "        state = env_reset(env)\n",
    "\n",
    "        self.state_shape = state.shape\n",
    "        self.state_size = np.prod(self.state_shape)\n",
    "        self.action_size = env.action_space.n\n",
    "        self.episodes_count = episodes_count\n",
    "        self.max_frame_in_episode = env.spec.max_episode_steps\n",
    "        self.max_memory_size = 10000\n",
    "        self.episodes_between_think = 1\n",
    "        \n",
    "        self.gamma = 0.95                # discount rate\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.epsilon_start = self.epsilon\n",
    "        self.epsilon_min = 0.0001        # min exploration rate\n",
    "        self.learning_rate = 0.1         # learning rate for algorithm\n",
    "        self.learning_rate_model = 0.01  # learning rate for model\n",
    "        \n",
    "        print(\"State shape {}, actions {}\".format(self.state_shape, self.action_size))\n",
    "\n",
    "    def decay_exploration_rate(self, episode):\n",
    "        # Linear exploration rate decay (lerp)\n",
    "#         self.epsilon = self.epsilon_start - \\\n",
    "#                       (self.epsilon_start - self.epsilon_min) * (float(frame) / self.frames_count)\n",
    "            \n",
    "        # Exponential rate decay\n",
    "        # y(0) = start\n",
    "        # y(1) = start * x\n",
    "        # y(2) = start * x^2\n",
    "        # y(steps) = start * x^steps = min => x = (min/start) ^ (1/steps)\n",
    "        # y(t) = start * x^t\n",
    "        self.epsilon = self.epsilon_start * \\\n",
    "                       math.pow( math.pow(self.epsilon_min / self.epsilon_start, 1.0 / self.episodes_count), episode )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action is added to input as OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionAsInputAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_len = self.params.state_size + self.params.action_size\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, input_len], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, 1], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([input_len, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, 1]))\n",
    "        b1 = tf.Variable(tf.random_normal([1]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'train_op': train_op,\n",
    "            'init': init\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state, frame))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "\n",
    "        X = np.resize(state, (1, self.params.state_size + self.params.action_size))\n",
    "        X[0, self.params.state_size:] = 0\n",
    "        \n",
    "        rewards = np.zeros((self.params.action_size))\n",
    "        for i in range(self.params.action_size):\n",
    "            X[0, self.params.state_size + i] = 1\n",
    "            rewards[i] = session.run(pred, {x: X})[0]\n",
    "            X[0, self.params.state_size + i] = 0\n",
    "        return np.argmax(rewards)\n",
    "    \n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((cnt, 1))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state, frame = self.memory[i]\n",
    "            inp = np.resize(state, (self.params.state_size + self.params.action_size))\n",
    "            inp[self.params.state_size:] = 0\n",
    "            inp[self.params.state_size + action] = 1\n",
    "            X[i], Y[i] = inp, reward\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Run with `params.episodes_between_think = 1`\n",
    "\n",
    "Karpathy: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "TF interpretation: https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name='Placeholder_x')\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name='Placeholder_y')\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "#         pred = tf.nn.softmax(pred)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.params.learning_rate_model, decay=0.99)\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model)\n",
    "    \n",
    "        gradients = optimizer.compute_gradients(cost, var_list=tf.trainable_variables())\n",
    "        train_op = optimizer.apply_gradients(gradients)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, feed_dict={x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, feed_dict={x: [state]})[0]\n",
    "            target[action] = reward\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        _ = session.run(train_op, {x: X, y: Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "#         train_op = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, {x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, {x: [state]})[0]\n",
    "            target[action] = reward + self.params.gamma * \\\n",
    "                             np.amax(session.run(pred, {x: [next_state]})[0])\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-23 21:04:49,613] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d4be00849f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# plt.imshow(env.render('rgb_array'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m print('Actions: {}, Observation space {}, {} parameters'.format(\n\u001b[0;32m---> 24\u001b[0;31m     env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "def preprocess_input_pong_v0(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# print(env.spec.max_episode_steps)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('Pong-v0')\n",
    "# env.my_preprocess_input = preprocess_input_pong_v0\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}, Observation space {}, {} parameters'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(210, 160, 3)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(env.spec.max_episode_steps)\n",
    "state = env.reset()\n",
    "env.step(1)\n",
    "# for i in range(10):\n",
    "#     state, reward, _, _ = env.step(1)\n",
    "# state, reward, _, _ = env.step(2)\n",
    "# state, reward, _, _ = env.step(3)\n",
    "# state, reward, _, _ = env.step(3)\n",
    "print(state.shape)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1224f6650>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h+ORQCnJoHWxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120ecb990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepro(state).reshape((80, 80)).shape\n",
    "plt.imshow(env.my_preprocess_input(state).reshape((80, 80)))\n",
    "# plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (2,), actions 3\n",
      "episode: 10/200, reward -200.0, frames 199, exploration rate: 0.69\n",
      "episode: 20/200, reward -200.0, frames 199, exploration rate: 0.44\n",
      "episode: 30/200, reward -200.0, frames 199, exploration rate: 0.28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-7f5c4e8b4700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/studying-ml/RL/training_methods.py\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(env, agent, params, normalize_rewards)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-1256d58f935a>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;31m# Captures the name of a node in an error status.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mbuild_results\u001b[0;34m(self, session, tensor_values)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# If the fetch was in the feeds, use the fed value, otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# use the returned value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m           \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env, episodes_count=200)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 500)\n",
    "\n",
    "agent = PolicyGradientAgent(params)\n",
    "\n",
    "# agent = DqnAgent(params)\n",
    "\n",
    "# agent = ActionAsInputAgent(params)\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "session = tf.Session()\n",
    "session.run(agent.model['init'])\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 8400 into shape (80,80)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-b840da19cb22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-167-9b82301f8d12>\u001b[0m in \u001b[0;36mshow\u001b[0;34m(env, agent, params, frames)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     img = plt.imshow(env.render(mode='rgb_array'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Pong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 8400 into shape (80,80)"
     ]
    }
   ],
   "source": [
    "show(env, agent, params, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
