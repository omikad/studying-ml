{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.optimizers import Adam\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display.display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, params, render, frames):\n",
    "    state = env.reset()\n",
    "    render_frames = []\n",
    "    for e in range(frames):\n",
    "        if render:\n",
    "            render_frames.append(env.render(mode = 'rgb_array'))\n",
    "        action = agent.act_greedy(state, e)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done or e == frames - 1:\n",
    "            print(\"score: {}\"\n",
    "                  .format(e))\n",
    "            break\n",
    "\n",
    "    if render:\n",
    "        env.render(close=True)\n",
    "        display_frames_as_gif(render_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(env, agent, params, frames):\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    state = env.reset()\n",
    "    frame = 0\n",
    "    for _ in range(frames):\n",
    "        img.set_data(env.render(mode='rgb_array'))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        action = agent.act_greedy(state, frame)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            frame = 0\n",
    "        else:\n",
    "            frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningParameters:\n",
    "    def __init__(self, state_shape, action_size, episodes_count):\n",
    "        self.state_shape = state_shape\n",
    "        self.state_size = np.prod(state_shape)\n",
    "        self.action_size = action_size\n",
    "        self.episodes_count = episodes_count\n",
    "        self.max_frame_in_episode = 500\n",
    "        self.max_memory_size = 10000\n",
    "        self.episodes_between_think = 5\n",
    "        \n",
    "        self.gamma = 0.95                # discount rate\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.epsilon_start = self.epsilon\n",
    "        self.epsilon_min = 0.0001        # min exploration rate\n",
    "        self.learning_rate = 0.1         # learning rate for algorithm\n",
    "        self.learning_rate_model = 0.01  # learning rate for model\n",
    "\n",
    "    def decay_exploration_rate(self, episode):\n",
    "        # Linear exploration rate decay (lerp)\n",
    "#         self.epsilon = self.epsilon_start - \\\n",
    "#                       (self.epsilon_start - self.epsilon_min) * (float(frame) / self.frames_count)\n",
    "            \n",
    "        # Exponential rate decay\n",
    "        # y(0) = start\n",
    "        # y(1) = start * x\n",
    "        # y(2) = start * x^2\n",
    "        # y(steps) = start * x^steps = min => x = (min/start) ^ (1/steps)\n",
    "        # y(t) = start * x^t\n",
    "        self.epsilon = self.epsilon_start * \\\n",
    "                       math.pow( math.pow(self.epsilon_min / self.epsilon_start, 1.0 / self.episodes_count), episode )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action is added to input as OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionAsInputAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_len = self.params.state_size + self.params.action_size\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, input_len], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, 1], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([input_len, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, 1]))\n",
    "        b1 = tf.Variable(tf.random_normal([1]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'train_op': train_op,\n",
    "            'init': init\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state, frame))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "\n",
    "        X = np.resize(state, (1, self.params.state_size + self.params.action_size))\n",
    "        X[0, self.params.state_size:] = 0\n",
    "        \n",
    "        rewards = np.zeros((self.params.action_size))\n",
    "        for i in range(self.params.action_size):\n",
    "            X[0, self.params.state_size + i] = 1\n",
    "            rewards[i] = session.run(pred, {x: X})[0]\n",
    "            X[0, self.params.state_size + i] = 0\n",
    "        return np.argmax(rewards)\n",
    "    \n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((cnt, 1))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state, frame = self.memory[i]\n",
    "            inp = np.resize(state, (self.params.state_size + self.params.action_size))\n",
    "            inp[self.params.state_size:] = 0\n",
    "            inp[self.params.state_size + action] = 1\n",
    "            X[i], Y[i] = inp, reward\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Run with `params.episodes_between_think = 1`\n",
    "\n",
    "Karpathy: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "TF interpretation: https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18\n",
    "\n",
    "TODO:\n",
    "* PG magic with normalizing discounted reward\n",
    "* PG magic with multiplying logprob to normalized discounted reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name='Placeholder_x')\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name='Placeholder_y')\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "\n",
    "#         optimizer = tf.train.RMSPropOptimizer(self.params.learning_rate_model, decay=0.99)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model)\n",
    "    \n",
    "        gradients = optimizer.compute_gradients(cost, var_list=tf.trainable_variables())\n",
    "        train_op = optimizer.apply_gradients(gradients)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, feed_dict={x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, feed_dict={x: [state]})[0]\n",
    "            target[action] = reward\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        _ = session.run(train_op, {x: X, y: Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "#         train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, {x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, {x: [state]})[0]\n",
    "            target[action] = reward + self.params.gamma * \\\n",
    "                             np.amax(session.run(pred, {x: [next_state]})[0])\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-16 10:59:33,578] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 2, Observation space (4,), 4 parameters\n"
     ]
    }
   ],
   "source": [
    "# print(env.spec.max_episode_steps)\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = gym.make('Breakout-v0')\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}, Observation space {}, {} parameters'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 25/500, reward 23.4, exploration rate: 0.65\n",
      "episode: 50/500, reward 29.0, exploration rate: 0.41\n",
      "episode: 75/500, reward 72.4, exploration rate: 0.26\n",
      "episode: 100/500, reward 85.2, exploration rate: 0.16\n",
      "episode: 125/500, reward 113.4, exploration rate: 0.1\n",
      "episode: 150/500, reward 109.7, exploration rate: 0.065\n",
      "episode: 175/500, reward 98.0, exploration rate: 0.041\n",
      "episode: 200/500, reward 151.9, exploration rate: 0.026\n",
      "episode: 225/500, reward 199.6, exploration rate: 0.016\n",
      "episode: 250/500, reward 164.4, exploration rate: 0.01\n",
      "episode: 275/500, reward 136.9, exploration rate: 0.0065\n",
      "episode: 300/500, reward 143.7, exploration rate: 0.0041\n",
      "episode: 325/500, reward 135.8, exploration rate: 0.0026\n",
      "episode: 350/500, reward 122.1, exploration rate: 0.0016\n",
      "episode: 375/500, reward 135.6, exploration rate: 0.001\n",
      "episode: 400/500, reward 148.0, exploration rate: 0.00065\n",
      "episode: 425/500, reward 134.0, exploration rate: 0.00041\n",
      "episode: 450/500, reward 144.4, exploration rate: 0.00026\n",
      "episode: 475/500, reward 149.1, exploration rate: 0.00016\n"
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env.observation_space.shape, env.action_space.n, 500)\n",
    "\n",
    "params.episodes_between_think = 1\n",
    "agent = PolicyGradientAgent(params)\n",
    "\n",
    "# agent = DqnAgent(params)\n",
    "\n",
    "# agent = ActionAsInputAgent(params)\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "session = tf.Session()\n",
    "session.run(agent.model['init'])\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 499\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, False, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzJL+D5ppW+UBmNAhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1139969d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(env, agent, params, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
