{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action is added to input as OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionAsInputAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_len = self.params.state_size + self.params.action_size\n",
    "\n",
    "        self.x = tf.placeholder(\"float\", [None, input_len], name=\"Placeholder_x\")\n",
    "        self.y = tf.placeholder(\"float\", [None, 1], name=\"Placeholder_y\")\n",
    "\n",
    "        h0 = tf.contrib.layers.fully_connected(\n",
    "            inputs=self.x,\n",
    "            num_outputs=20,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=h0,\n",
    "            num_outputs=1,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.cost = tf.nn.l2_loss(self.pred - self.y)\n",
    "        self.train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "            .minimize(self.cost)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state, frame))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        X = np.resize(state, (1, self.params.state_size + self.params.action_size))\n",
    "        X[0, self.params.state_size:] = 0\n",
    "        \n",
    "        rewards = np.zeros((self.params.action_size))\n",
    "        for i in range(self.params.action_size):\n",
    "            X[0, self.params.state_size + i] = 1\n",
    "            rewards[i] = session.run(self.pred, {self.x: X})[0]\n",
    "            X[0, self.params.state_size + i] = 0\n",
    "        return np.argmax(rewards)\n",
    "    \n",
    "    def think(self, batch_size, episode):\n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((cnt, 1))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state, frame = self.memory[i]\n",
    "            inp = np.resize(state, (self.params.state_size + self.params.action_size))\n",
    "            inp[self.params.state_size:] = 0\n",
    "            inp[self.params.state_size + action] = 1\n",
    "            X[i], Y[i] = inp, reward\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            batch_x = X[batch_indexes]\n",
    "            batch_y = Y[batch_indexes]\n",
    "            _ = session.run(self.train_op, {self.x: batch_x, self.y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Run with `params.episodes_between_think = 1`\n",
    "\n",
    "Karpathy: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "TF interpretation: https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.x = tf.placeholder(\"float\", [None, self.params.state_size], name='Placeholder_x')\n",
    "        self.y = tf.placeholder(\"float\", [None, self.params.action_size], name='Placeholder_y')\n",
    "\n",
    "        h0 = tf.contrib.layers.fully_connected(\n",
    "            inputs=self.x,\n",
    "            num_outputs=20,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=h0,\n",
    "            num_outputs=self.params.action_size,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.cost = tf.nn.l2_loss(self.pred - self.y)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.params.learning_rate_model, decay=0.99)\n",
    "    \n",
    "        gradients = optimizer.compute_gradients(self.cost, var_list=tf.trainable_variables())\n",
    "        self.train_op = optimizer.apply_gradients(gradients)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        act_values = session.run(self.pred, feed_dict={self.x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size, episode):\n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(self.pred, feed_dict={self.x: [state]})[0]\n",
    "            target[action] = reward\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        _ = session.run(self.train_op, {self.x: X, self.y: Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_next_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_actions = np.zeros((2 * self.params.max_memory_size), dtype=np.int32)\n",
    "        self.memory_rewards = np.zeros((2 * self.params.max_memory_size, 1))\n",
    "        self.cnt = 0\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_x\")\n",
    "        self.y = tf.placeholder(\"float\", [None, self.params.action_size], name=\"Placeholder_y\")\n",
    "\n",
    "        # Pong:\n",
    "#         x = tf.reshape(self.x, (tf.shape(self.x)[0], 19, 16, 4))\n",
    "#         conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "#         flattened = tf.contrib.layers.flatten(conv1)\n",
    "\n",
    "        # Breakout:\n",
    "        x = tf.reshape(self.x, (tf.shape(self.x)[0], 80, 70, 4))\n",
    "        conv1 = tf.contrib.layers.conv2d(x, 16, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1, 32, 4, 2, activation_fn=tf.nn.relu)\n",
    "        flattened = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Cartpole:\n",
    "#         flattened = self.x\n",
    "\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 128,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=fc1,\n",
    "            num_outputs=self.params.action_size,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "#         self.cost = tf.nn.l2_loss(self.pred - self.y)\n",
    "        self.losses = tf.squared_difference(self.pred, self.y)\n",
    "        self.cost = tf.reduce_mean(self.losses)\n",
    "\n",
    "        self.train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "            .minimize(self.cost)\n",
    "#         train_op = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model).minimize(cost)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory_states[self.cnt] = state\n",
    "        self.memory_next_states[self.cnt] = next_state\n",
    "        self.memory_actions[self.cnt] = action\n",
    "        self.memory_rewards[self.cnt] = reward\n",
    "        self.cnt += 1\n",
    "        \n",
    "        if self.cnt == 2 * self.params.max_memory_size:\n",
    "            n = self.params.max_memory_size\n",
    "            self.memory_states[:n] = self.memory_states[-n:]\n",
    "            self.memory_next_states[:n] = self.memory_next_states[-n:]\n",
    "            self.memory_actions[:n] = self.memory_actions[-n:]\n",
    "            self.memory_rewards[:n] = self.memory_rewards[-n:]\n",
    "            self.cnt = n\n",
    "\n",
    "    def act(self, session, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(session, state, frame)\n",
    "    \n",
    "    def act_greedy(self, session, state, frame):\n",
    "        act_values = session.run(self.pred, {self.x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, session, batch_size, episode):\n",
    "        if self.cnt < 2000:\n",
    "            return\n",
    "        \n",
    "        cnt = self.cnt\n",
    "        \n",
    "        values = session.run(self.pred, {self.x: self.memory_states[:cnt]})\n",
    "        nextValues = session.run(self.pred, {self.x: self.memory_next_states[:cnt]})\n",
    "        \n",
    "        values[np.arange(cnt), self.memory_actions[:cnt]] = \\\n",
    "            self.memory_rewards[:cnt, 0] + self.params.gamma * np.amax(nextValues, axis=1)\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            batch_x = self.memory_states[batch_indexes]\n",
    "            batch_y = values[batch_indexes]\n",
    "            _ = session.run(self.train_op, {self.x: batch_x, self.y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-03 01:11:58,976] Making new env: Breakout-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4\n",
      "Raw observation space: (210, 160, 3)\n",
      "Max episode steps: 10000\n",
      "Preprocessed observation space: (22400,)\n",
      "Parameters: 22400\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "# env_state_observer = EnvStateObserver(lambda x: x, concat_states_count=3)\n",
    "\n",
    "# env = gym.make('Pong-v0')\n",
    "# env_state_observer = EnvStateObserver(preprocess_input_pong_v0, concat_states_count=4)\n",
    "\n",
    "env = gym.envs.make(\"Breakout-v0\")\n",
    "env_state_observer = EnvStateObserver(preprocess_input_breakout_v0, concat_states_count=4)\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "sample_state = env_state_observer.env_reset(env)\n",
    "print('Actions: {}'.format(env.action_space.n))\n",
    "print('Raw observation space: {}'.format(env.observation_space.shape))\n",
    "print('Max episode steps: {}'.format(env.spec.max_episode_steps))\n",
    "print('Preprocessed observation space: {}'.format(sample_state.shape))\n",
    "print('Parameters: {}'.format(np.prod(sample_state.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((22400,), 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    22400.000000\n",
       "mean        -0.000032\n",
       "std          1.000508\n",
       "min         -0.526482\n",
       "25%         -0.526482\n",
       "50%         -0.526482\n",
       "75%         -0.526482\n",
       "max          2.319368\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAAD8CAYAAACFDhMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cfcb63110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env_state_observer.env_reset(env)\n",
    "state, reward, next_state, done = env_state_observer.env_step(env, 1)\n",
    "print(state.shape, reward)\n",
    "plt.imshow(state.reshape((80, 70, 4))[:,:,3], cmap='Greys')\n",
    "pd.Series(state).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (22400,), actions 4\n",
      "Run: 1\n",
      "Loading model checkpoint logs/dqn/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from logs/dqn/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-03 01:12:16,198] Restoring parameters from logs/dqn/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1250/25000, reward 1.8, frames 383, exploration rate: 0.86\n",
      "episode: 2500/25000, reward 2.2, frames 243, exploration rate: 0.74\n",
      "episode: 3750/25000, reward 2.6, frames 251, exploration rate: 0.64\n",
      "episode: 5000/25000, reward 1.1, frames 209, exploration rate: 0.55\n",
      "episode: 6250/25000, reward 1.0, frames 199, exploration rate: 0.47\n",
      "episode: 7500/25000, reward 1.8, frames 378, exploration rate: 0.41\n",
      "episode: 8750/25000, reward 0.7, frames 223, exploration rate: 0.35\n",
      "episode: 10000/25000, reward 2.0, frames 333, exploration rate: 0.3\n",
      "episode: 11250/25000, reward 1.1, frames 332, exploration rate: 0.26\n",
      "episode: 12500/25000, reward 1.6, frames 310, exploration rate: 0.22\n",
      "episode: 13750/25000, reward 1.6, frames 166, exploration rate: 0.19\n",
      "episode: 15000/25000, reward 1.2, frames 337, exploration rate: 0.17\n",
      "episode: 16250/25000, reward 0.8, frames 447, exploration rate: 0.14\n",
      "episode: 17500/25000, reward 1.3, frames 427, exploration rate: 0.12\n",
      "episode: 18750/25000, reward 0.9, frames 228, exploration rate: 0.11\n",
      "episode: 20000/25000, reward 1.2, frames 495, exploration rate: 0.091\n",
      "episode: 21250/25000, reward 2.2, frames 473, exploration rate: 0.078\n",
      "episode: 22500/25000, reward 1.3, frames 438, exploration rate: 0.067\n",
      "episode: 23750/25000, reward 1.1, frames 162, exploration rate: 0.058\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8128b16add23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n\u001b[0;32m---> 33\u001b[0;31m                                               normalize_rewards=True)\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(session, saver, env, agent, env_state_observer, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-182f45121600>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, session, batch_size, episode)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "params = LearningParameters(env, env_state_observer.env_reset(env), episodes_count=25000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 1000)\n",
    "params.epsilon_min = 0.05\n",
    "params.max_memory_size = 2000\n",
    "params.pong_reset_discounted_reward = False\n",
    "# agent = PolicyGradientAgent(params)\n",
    "agent = DqnAgent(params)\n",
    "# agent = ActionAsInputAgent(params)\n",
    "\n",
    "run_name += 1\n",
    "print('Run: ' + str(run_name))\n",
    "\n",
    "saver = TfSaver('logs/dqn')\n",
    "\n",
    "# Train on GPU\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.7\n",
    "config.operation_timeout_in_ms=60000\n",
    "\n",
    "# Train on CPU\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "\n",
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.load_latest_checkpoint(session)\n",
    "    \n",
    "#     tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "    agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n",
    "                                              normalize_rewards=True)\n",
    "    # agent, rewards = train_reward_is_time(env, agent, params)\n",
    "    # agent, rewards = train(env, agent, params)\n",
    "    plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continue learning\n",
    "params.episodes_count = 10000\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    evaluate(session, env, agent, env_state_observer, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAAD8CAYAAACFDhMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYZRaoRhlBrxf8w02G3HqeUuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd03e6c110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    show(session, env, agent, env_state_observer, params, 200, width=80, height=70, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save rewards/model\n",
    "pd.DataFrame(rewards).to_csv('models/rewards_40K_50K.csv', header=None)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, 'models/Pong PolicyGradient', global_step=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
