{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(env, agent, params, frames, width, height, greedy=True):\n",
    "#     img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    state = env_reset(env)\n",
    "    img = plt.imshow(state.reshape(width, height))\n",
    "    frame = 0\n",
    "    for _ in range(frames):\n",
    "#         img.set_data(env.render(mode='rgb_array'))\n",
    "        img.set_data(state.reshape(width, height))\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        action = agent.act_greedy(state, frame) if greedy else np.random.randint(0, params.action_size)\n",
    "        state, reward, done, _ = env_step(env, action)\n",
    "        if done:\n",
    "            state = env_reset(env)\n",
    "            frame = 0\n",
    "        else:\n",
    "            frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningParameters:\n",
    "    def __init__(self, env, episodes_count):\n",
    "        state = env_reset(env)\n",
    "\n",
    "        self.state_shape = state.shape\n",
    "        self.state_size = np.prod(self.state_shape)\n",
    "        self.action_size = 4  # !!!!!!!!!!!!!!!!!!!!!!!!  env.action_space.n\n",
    "        self.episodes_count = episodes_count\n",
    "        self.max_frame_in_episode = env.spec.max_episode_steps\n",
    "        self.max_memory_size = 10000\n",
    "        self.episodes_between_think = 1\n",
    "        \n",
    "        self.gamma = 0.95                # discount rate\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.epsilon_start = self.epsilon\n",
    "        self.epsilon_min = 0.0001        # min exploration rate\n",
    "        self.learning_rate = 0.1         # learning rate for algorithm\n",
    "        self.learning_rate_model = 0.01  # learning rate for model\n",
    "        \n",
    "        print(\"State shape {}, actions {}\".format(self.state_shape, self.action_size))\n",
    "\n",
    "    def decay_exploration_rate(self, episode):\n",
    "        # Linear exploration rate decay (lerp)\n",
    "#         self.epsilon = self.epsilon_start - \\\n",
    "#                       (self.epsilon_start - self.epsilon_min) * (float(frame) / self.frames_count)\n",
    "            \n",
    "        # Exponential rate decay\n",
    "        # y(0) = start\n",
    "        # y(1) = start * x\n",
    "        # y(2) = start * x^2\n",
    "        # y(steps) = start * x^steps = min => x = (min/start) ^ (1/steps)\n",
    "        # y(t) = start * x^t\n",
    "        self.epsilon = self.epsilon_start * \\\n",
    "                       math.pow( math.pow(self.epsilon_min / self.epsilon_start, 1.0 / self.episodes_count), episode )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action is added to input as OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActionAsInputAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        input_len = self.params.state_size + self.params.action_size\n",
    "\n",
    "        x = tf.placeholder(\"float\", [None, input_len], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, 1], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([input_len, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, 1]))\n",
    "        b1 = tf.Variable(tf.random_normal([1]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'train_op': train_op,\n",
    "            'init': init\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state, frame))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "\n",
    "        X = np.resize(state, (1, self.params.state_size + self.params.action_size))\n",
    "        X[0, self.params.state_size:] = 0\n",
    "        \n",
    "        rewards = np.zeros((self.params.action_size))\n",
    "        for i in range(self.params.action_size):\n",
    "            X[0, self.params.state_size + i] = 1\n",
    "            rewards[i] = session.run(pred, {x: X})[0]\n",
    "            X[0, self.params.state_size + i] = 0\n",
    "        return np.argmax(rewards)\n",
    "    \n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((cnt, 1))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state, frame = self.memory[i]\n",
    "            inp = np.resize(state, (self.params.state_size + self.params.action_size))\n",
    "            inp[self.params.state_size:] = 0\n",
    "            inp[self.params.state_size + action] = 1\n",
    "            X[i], Y[i] = inp, reward\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "\n",
    "Run with `params.episodes_between_think = 1`\n",
    "\n",
    "Karpathy: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5\n",
    "\n",
    "TF interpretation: https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name='Placeholder_x')\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name='Placeholder_y')\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "#         pred = tf.nn.softmax(pred)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.params.learning_rate_model, decay=0.99)\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model)\n",
    "    \n",
    "        gradients = optimizer.compute_gradients(cost, var_list=tf.trainable_variables())\n",
    "        train_op = optimizer.apply_gradients(gradients)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, feed_dict={x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, feed_dict={x: [state]})[0]\n",
    "            target[action] = reward\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        _ = session.run(train_op, {x: X, y: Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_x\")\n",
    "        y = tf.placeholder(\"float\", [None, self.params.action_size], name=\"Placeholder_y\")\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.params.state_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(pred - y)\n",
    "        train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99).minimize(cost)\n",
    "#         train_op = tf.train.AdamOptimizer(learning_rate=self.params.learning_rate_model).minimize(cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'pred': pred,\n",
    "            'cost': cost,\n",
    "            'init': init,\n",
    "            'train_op': train_op\n",
    "        }\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        x = self.model['x']\n",
    "        pred = self.model['pred']\n",
    "        act_values = session.run(pred, {x: [state]})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size):\n",
    "        x = self.model['x']\n",
    "        y = self.model['y']\n",
    "        pred = self.model['pred']\n",
    "        train_op = self.model['train_op']\n",
    "        cost = self.model['cost']\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.params.state_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            target = session.run(pred, {x: [state]})[0]\n",
    "            target[action] = reward + self.params.gamma * \\\n",
    "                             np.amax(session.run(pred, {x: [next_state]})[0])\n",
    "            X[i], Y[i] = state, target\n",
    "\n",
    "        for e in range(1):\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, cnt, batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(train_op, {x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-25 21:08:03,043] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 6, Observation space (210, 160, 3), 100800 parameters\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input_pong_v0(I):\n",
    "    I = I[35:195] # crop\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I = I[::2,::2,0] + I[1::2,::2,0] + I[::2,1::2,0] + I[1::2,1::2,0]\n",
    "    I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "    I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def preprocess_input_breakout_v0(I):\n",
    "    I = I[35:195, 10:150]  # crop to (160, 140, 3)\n",
    "    I = (I[:,:,0] + I[:,:,1] + I[:,:,2]) / 3\n",
    "    I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "# print(env.spec.max_episode_steps)\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('Breakout-v0')\n",
    "# env.my_preprocess_input = preprocess_input_breakout_v0\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env.my_preprocess_input = preprocess_input_pong_v0\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}, Observation space {}, {} parameters'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(210, 160, 3)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(env.spec.max_episode_steps)\n",
    "state = env.reset()\n",
    "env.step(1)\n",
    "# for i in range(10):\n",
    "#     state, reward, _, _ = env.step(1)\n",
    "# state, reward, _, _ = env.step(2)\n",
    "# state, reward, _, _ = env.step(3)\n",
    "# state, reward, _, _ = env.step(3)\n",
    "print(state.shape)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((400,), 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd14bc7a90>"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZEBIajIgJDUZEJKaDAhJTf8H95roFeuEQT8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd14cafd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# env.reset()\n",
    "state, reward, next_state, done = env.step(2)\n",
    "state = env.my_preprocess_input(state)\n",
    "print(state.shape, reward)\n",
    "# prepro(state).reshape((80, 80)).shape\n",
    "plt.imshow(state.reshape((20, 20)), cmap='Greys')\n",
    "# plt.imshow(env.render('rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (400,), actions 4\n",
      "episode: 500/10000, reward -20.6, frames 1088, exploration rate: 0.89\n",
      "episode: 1000/10000, reward -20.7, frames 1169, exploration rate: 0.79\n",
      "episode: 1500/10000, reward -20.8, frames 1109, exploration rate: 0.71\n",
      "episode: 2000/10000, reward -20.4, frames 1304, exploration rate: 0.63\n",
      "episode: 2500/10000, reward -20.8, frames 1099, exploration rate: 0.56\n",
      "episode: 3000/10000, reward -20.8, frames 1010, exploration rate: 0.5\n",
      "episode: 3500/10000, reward -20.8, frames 1019, exploration rate: 0.45\n",
      "episode: 4000/10000, reward -20.9, frames 1180, exploration rate: 0.4\n",
      "episode: 4500/10000, reward -20.8, frames 1172, exploration rate: 0.35\n",
      "episode: 5000/10000, reward -20.3, frames 1096, exploration rate: 0.32\n",
      "episode: 5500/10000, reward -20.9, frames 1000, exploration rate: 0.28\n",
      "episode: 6000/10000, reward -20.9, frames 1029, exploration rate: 0.25\n",
      "episode: 6500/10000, reward -20.9, frames 1088, exploration rate: 0.22\n",
      "episode: 7000/10000, reward -21.0, frames 1018, exploration rate: 0.2\n",
      "episode: 7500/10000, reward -20.9, frames 1040, exploration rate: 0.18\n",
      "episode: 8000/10000, reward -20.9, frames 1419, exploration rate: 0.16\n",
      "episode: 8500/10000, reward -21.0, frames 1021, exploration rate: 0.14\n",
      "episode: 9000/10000, reward -20.8, frames 1061, exploration rate: 0.13\n",
      "episode: 9500/10000, reward -21.0, frames 1095, exploration rate: 0.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-365-27be8b5ef8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'init'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.py\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(env, agent, params, normalize_rewards)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_frame_in_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.py\u001b[0m in \u001b[0;36menv_step\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_preprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/gym/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/gym/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/gym/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/gym/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env, episodes_count=10000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 100000)\n",
    "params.epsilon_min = 0.1\n",
    "\n",
    "# agent = PolicyGradientAgent(params)\n",
    "\n",
    "agent = DqnAgent(params)\n",
    "\n",
    "# agent = ActionAsInputAgent(params)\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "session = tf.Session(config=config)\n",
    "session.run(agent.model['init'])\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd14c12110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(env, agent, params, 100, width=20, height=20, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
