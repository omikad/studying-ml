{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-23 18:24:46,748] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 2, Observation space (4,), parameters 4, max episode steps 500\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('Breakout-v0')\n",
    "# env.my_preprocess_input = preprocess_input_breakout_v0\n",
    "\n",
    "# env = gym.make('Pong-v0')  # Reset TD reward by non-null values\n",
    "# env.my_preprocess_input = preprocess_input_pong_v0\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}, Observation space {}, parameters {}, max episode steps {}'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape), env.spec.max_episode_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self._build_actor_model()\n",
    "        self._build_critic_model()\n",
    "\n",
    "    def _build_actor_model(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.actor_y = tf.placeholder(\"float\", [None], name=\"Target\")\n",
    "            self.actor_action = tf.placeholder(tf.int32, [None], name=\"Action\")\n",
    "\n",
    "            h0 = tf.contrib.layers.fully_connected(\n",
    "                inputs=self.actor_x,\n",
    "                num_outputs=20,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            h1 = tf.contrib.layers.fully_connected(\n",
    "                inputs=h0,\n",
    "                num_outputs=self.params.action_size,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.actor_action_probs = tf.nn.softmax(h1)\n",
    "\n",
    "            actions_ohe = tf.one_hot(self.actor_action, self.params.action_size)\n",
    "            self.picked_action_probs = tf.reduce_sum(self.actor_action_probs * actions_ohe, axis=1)\n",
    "\n",
    "            self.actor_cost = tf.reduce_mean(-tf.log(1e-6 + self.picked_action_probs) * self.actor_y)\n",
    "            self.actor_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.actor_cost)\n",
    "\n",
    "    def _build_critic_model(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.critic_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.critic_y = tf.placeholder(\"float\", [None], name=\"Target\")\n",
    "\n",
    "            h0 = tf.contrib.layers.fully_connected(\n",
    "                inputs=self.critic_x,\n",
    "                num_outputs=20,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            self.critic_value = tf.contrib.layers.fully_connected(\n",
    "                inputs=h0,\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_cost = tf.reduce_mean(tf.squared_difference(self.critic_value, self.critic_y))\n",
    "            self.critic_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.critic_cost)\n",
    "            \n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        act_probs = session.run(self.actor_action_probs, {self.actor_x: [state]})[0]\n",
    "        return np.random.choice(np.arange(len(act_probs)), p=act_probs)\n",
    "\n",
    "    def think(self, batch_size, episode):\n",
    "        cnt = len(self.memory)\n",
    "        States = np.zeros((cnt, self.params.state_size))\n",
    "        ActorY = np.zeros((cnt))\n",
    "        Actions = np.zeros((cnt), dtype=np.int32)\n",
    "        CriticY = np.zeros((cnt))\n",
    "        \n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            \n",
    "            values = session.run(self.critic_value, {self.critic_x: [state, next_state]})\n",
    "            \n",
    "            td = reward + self.params.gamma * values[1][0]\n",
    "            ad = td - values[0][0]\n",
    "            \n",
    "            States[i], ActorY[i], CriticY[i], Actions[i] = state, ad, td, action\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            states = States[batch_indexes]\n",
    "            actorys = ActorY[batch_indexes]\n",
    "            criticys = CriticY[batch_indexes]\n",
    "            actions = Actions[batch_indexes]\n",
    "            \n",
    "            _ = session.run(self.actor_train_op,\n",
    "                            {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n",
    "            _ = session.run(self.critic_train_op,\n",
    "                            {self.critic_x: states, self.critic_y: criticys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (304,), actions 6\n",
      "Run: 27\n",
      "episode: 500/10000, reward -12.5, frames 1000, exploration rate: 0.86\n",
      "episode: 1000/10000, reward -14.9, frames 1000, exploration rate: 0.74\n",
      "episode: 1500/10000, reward -17.2, frames 1000, exploration rate: 0.64\n",
      "episode: 2000/10000, reward -17.8, frames 1000, exploration rate: 0.55\n",
      "episode: 2500/10000, reward -17.1, frames 1000, exploration rate: 0.47\n",
      "episode: 3000/10000, reward -14.1, frames 1000, exploration rate: 0.41\n",
      "episode: 3500/10000, reward -13.9, frames 1000, exploration rate: 0.35\n",
      "episode: 4000/10000, reward -16.0, frames 1000, exploration rate: 0.3\n",
      "episode: 4500/10000, reward -17.6, frames 1000, exploration rate: 0.26\n",
      "episode: 5000/10000, reward -17.5, frames 1000, exploration rate: 0.22\n",
      "episode: 5500/10000, reward -17.1, frames 1000, exploration rate: 0.19\n",
      "episode: 6000/10000, reward -17.1, frames 1000, exploration rate: 0.17\n",
      "episode: 6500/10000, reward -18.1, frames 1000, exploration rate: 0.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-0f0b40ab0343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.py\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(env, agent, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-e1345d97a72a>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, batch_size, episode)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mtd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env, episodes_count=10000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 1000)\n",
    "params.epsilon_min = 0.05\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "    tf.reset_default_graph()\n",
    "    if 'tf_writer' in locals():\n",
    "        tf_writer.close()\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "session = tf.Session(config=config)\n",
    "run_name += 1\n",
    "print('Run: ' + str(run_name))\n",
    "\n",
    "agent = ActorCriticAgent(params)\n",
    "\n",
    "tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4e148a4039f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Continue learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.py\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(env, agent, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-20af59c7f189>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, batch_size, episode)\u001b[0m\n\u001b[1;32m     95\u001b[0m                             {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n\u001b[1;32m     96\u001b[0m             _ = session.run(self.critic_train_op,\n\u001b[0;32m---> 97\u001b[0;31m                             {self.critic_x: states, self.critic_y: criticys})\n\u001b[0m",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue learning\n",
    "params.episodes_count = 500\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -500.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOYAAAD8CAYAAABjJ9hGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYEoNMphSgwym1KD/A3Xsy/GJF1rhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78b042ba10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show(env, agent, params, 500, width=19, height=16, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/Pong ActorCriticAgent-9000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save rewards/model\n",
    "# pd.DataFrame(rewards).to_csv('models/rewards_40K_50K.csv', header=None)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, 'models/Pong ActorCriticAgent', global_step=9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
