{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-29 09:37:17,641] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 6\n",
      "Raw observation space: (210, 160, 3)\n",
      "Max episode steps: 10000\n",
      "Preprocessed observation space: (1216,)\n",
      "Parameters: 1216\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "# env_state_observer = EnvStateObserver(lambda x: x, concat_states_count=3)\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env_state_observer = EnvStateObserver(preprocess_input_pong_v0, concat_states_count=4)\n",
    "sample_state = env_state_observer.env_reset(env)\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}'.format(env.action_space.n))\n",
    "print('Raw observation space: {}'.format(env.observation_space.shape))\n",
    "print('Max episode steps: {}'.format(env.spec.max_episode_steps))\n",
    "print('Preprocessed observation space: {}'.format(sample_state.shape))\n",
    "print('Parameters: {}'.format(np.prod(sample_state.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_next_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_actions = np.zeros((2 * self.params.max_memory_size))\n",
    "        self.memory_rewards = np.zeros((2 * self.params.max_memory_size, 1))\n",
    "        self.cnt = 0\n",
    "        self._build_actor_model()\n",
    "        self._build_critic_model()\n",
    "\n",
    "    def _build_actor_model(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.actor_y = tf.placeholder(\"float\", [None, 1], name=\"Target\")\n",
    "            self.actor_action = tf.placeholder(tf.int32, [None,], name=\"Action\")\n",
    "\n",
    "            x = tf.reshape(self.actor_x, (tf.shape(self.actor_x)[0], 19, 16, 4))\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv1)\n",
    "            \n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 32,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, self.params.action_size,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.actor_action_probs = tf.nn.softmax(fc2)\n",
    "\n",
    "            actions_ohe = tf.one_hot(self.actor_action, depth=self.params.action_size)\n",
    "            self.picked_action_probs = tf.reduce_sum(self.actor_action_probs * actions_ohe, axis=1, keep_dims=True)\n",
    "\n",
    "            self.actor_cost = tf.reduce_mean(-tf.log(1e-6 + self.picked_action_probs) * self.actor_y)\n",
    "            self.actor_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.actor_cost)\n",
    "\n",
    "    def _build_critic_model(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.critic_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.critic_y = tf.placeholder(\"float\", [None, 1], name=\"Target\")\n",
    "\n",
    "            x = tf.reshape(self.critic_x, (tf.shape(self.critic_x)[0], 19, 16, 4))\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv1)\n",
    "            \n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 32,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_value = tf.contrib.layers.fully_connected(fc1, 1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_cost = tf.reduce_mean(tf.squared_difference(self.critic_value, self.critic_y))\n",
    "            self.critic_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.critic_cost)\n",
    "            \n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory_states[self.cnt] = state\n",
    "        self.memory_next_states[self.cnt] = next_state\n",
    "        self.memory_actions[self.cnt] = action\n",
    "        self.memory_rewards[self.cnt] = reward\n",
    "        self.cnt += 1\n",
    "        \n",
    "        if self.cnt == 2 * self.params.max_memory_size:\n",
    "            n = self.params.max_memory_size\n",
    "            self.memory_states[:n] = self.memory_states[-n:]\n",
    "            self.memory_next_states[:n] = self.memory_next_states[-n:]\n",
    "            self.memory_actions[:n] = self.memory_actions[-n:]\n",
    "            self.memory_rewards[:n] = self.memory_rewards[-n:]\n",
    "            self.cnt = n\n",
    "\n",
    "    def act(self, session, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(session, state, frame)\n",
    "    \n",
    "    def act_greedy(self, session, state, frame):\n",
    "        act_probs = session.run(self.actor_action_probs, {self.actor_x: [state]})[0]\n",
    "        return np.random.choice(np.arange(len(act_probs)), p=act_probs)\n",
    "\n",
    "    def think(self, session, batch_size, episode):\n",
    "        cnt = self.cnt\n",
    "        \n",
    "        values = session.run(self.critic_value, {self.critic_x: self.memory_states[:cnt]})\n",
    "        nextValues = session.run(self.critic_value, {self.critic_x: self.memory_next_states[:cnt]})\n",
    "        \n",
    "        criticY = self.memory_rewards[:cnt] + self.params.gamma * nextValues[:cnt]\n",
    "        actorY = criticY - values\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            states = self.memory_states[batch_indexes]\n",
    "            actorys = actorY[batch_indexes]\n",
    "            criticys = criticY[batch_indexes]\n",
    "            actions = self.memory_actions[batch_indexes]\n",
    "            \n",
    "            _ = session.run(self.actor_train_op,\n",
    "                            {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n",
    "            _ = session.run(self.critic_train_op,\n",
    "                            {self.critic_x: states, self.critic_y: criticys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (1216,), actions 6\n",
      "Run: 3\n",
      "episode: 500/10000, reward -16.9, frames 1000, exploration rate: 0.86\n",
      "episode: 1000/10000, reward -16.6, frames 1000, exploration rate: 0.74\n",
      "episode: 1500/10000, reward -15.9, frames 1000, exploration rate: 0.64\n",
      "episode: 2000/10000, reward -17.2, frames 1000, exploration rate: 0.55\n",
      "episode: 2500/10000, reward -17.4, frames 1000, exploration rate: 0.47\n",
      "episode: 3000/10000, reward -16.7, frames 1000, exploration rate: 0.41\n",
      "episode: 3500/10000, reward -18.1, frames 1000, exploration rate: 0.35\n",
      "episode: 4000/10000, reward -17.2, frames 1000, exploration rate: 0.3\n",
      "episode: 4500/10000, reward -18.3, frames 1000, exploration rate: 0.26\n",
      "episode: 5000/10000, reward -16.7, frames 1000, exploration rate: 0.22\n",
      "episode: 5500/10000, reward -18.3, frames 1000, exploration rate: 0.19\n",
      "episode: 6000/10000, reward -16.5, frames 1000, exploration rate: 0.17\n",
      "episode: 6500/10000, reward -17.5, frames 1000, exploration rate: 0.14\n",
      "episode: 7000/10000, reward -18.1, frames 1000, exploration rate: 0.12\n",
      "episode: 7500/10000, reward -17.6, frames 1000, exploration rate: 0.11\n",
      "episode: 8000/10000, reward -18.7, frames 1000, exploration rate: 0.091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-26c4855937d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n\u001b[0;32m---> 28\u001b[0;31m                                               normalize_rewards=True)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(session, saver, env, agent, env_state_observer, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c8daa34d5822>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, session, batch_size, episode)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             _ = session.run(self.actor_train_op,\n\u001b[0;32m--> 100\u001b[0;31m                             {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n\u001b[0m\u001b[1;32m    101\u001b[0m             _ = session.run(self.critic_train_op,\n\u001b[1;32m    102\u001b[0m                             {self.critic_x: states, self.critic_y: criticys})\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "params = LearningParameters(env, env_state_observer.env_reset(env), episodes_count=10000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 1000)\n",
    "params.epsilon_min = 0.05\n",
    "params.max_memory_size = 20000\n",
    "params.pong_reset_discounted_reward = True\n",
    "agent = ActorCriticAgent(params)\n",
    "\n",
    "run_name += 1\n",
    "print('Run: ' + str(run_name))\n",
    "\n",
    "saver = TfSaver('logs')\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "config.operation_timeout_in_ms=60000\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# config = tf.ConfigProto()\n",
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.load_latest_checkpoint(session)\n",
    "    \n",
    "#     tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "    agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n",
    "                                              normalize_rewards=True)\n",
    "    # agent, rewards = train_reward_is_time(env, agent, params)\n",
    "    # agent, rewards = train(env, agent, params)\n",
    "    plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -3.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    evaluate(session, env, agent, env_state_observer, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOYAAAD8CAYAAABjJ9hGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8c6fe4d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    show(session, env, agent, env_state_observer, params, 500, width=19, height=16, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/Pong ActorCriticAgent-9000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save rewards/model\n",
    "# pd.DataFrame(rewards).to_csv('models/rewards_40K_50K.csv', header=None)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, 'models/Pong ActorCriticAgent', global_step=9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
