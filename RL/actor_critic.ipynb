{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-01 08:49:40,214] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 6\n",
      "Raw observation space: (210, 160, 3)\n",
      "Max episode steps: 10000\n",
      "Preprocessed observation space: (1216,)\n",
      "Parameters: 1216\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "# env_state_observer = EnvStateObserver(lambda x: x, concat_states_count=3)\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env_state_observer = EnvStateObserver(preprocess_input_pong_v0, concat_states_count=4)\n",
    "sample_state = env_state_observer.env_reset(env)\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}'.format(env.action_space.n))\n",
    "print('Raw observation space: {}'.format(env.observation_space.shape))\n",
    "print('Max episode steps: {}'.format(env.spec.max_episode_steps))\n",
    "print('Preprocessed observation space: {}'.format(sample_state.shape))\n",
    "print('Parameters: {}'.format(np.prod(sample_state.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_next_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_actions = np.zeros((2 * self.params.max_memory_size))\n",
    "        self.memory_rewards = np.zeros((2 * self.params.max_memory_size, 1))\n",
    "        self.cnt = 0\n",
    "        self._build_actor_model()\n",
    "        self._build_critic_model()\n",
    "\n",
    "    def _build_actor_model(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.actor_y = tf.placeholder(\"float\", [None, 1], name=\"Target\")\n",
    "            self.actor_action = tf.placeholder(tf.int32, [None,], name=\"Action\")\n",
    "\n",
    "            x = tf.reshape(self.actor_x, (tf.shape(self.actor_x)[0], 19, 16, 4))\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv1)\n",
    "            \n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 32,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, self.params.action_size,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.actor_action_probs = tf.nn.softmax(fc2)\n",
    "\n",
    "            actions_ohe = tf.one_hot(self.actor_action, depth=self.params.action_size)\n",
    "            self.picked_action_probs = tf.reduce_sum(self.actor_action_probs * actions_ohe, axis=1, keep_dims=True)\n",
    "\n",
    "            self.actor_cost = tf.reduce_mean(-tf.log(1e-6 + self.picked_action_probs) * self.actor_y)\n",
    "            self.actor_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.actor_cost)\n",
    "\n",
    "    def _build_critic_model(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.critic_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.critic_y = tf.placeholder(\"float\", [None, 1], name=\"Target\")\n",
    "\n",
    "            x = tf.reshape(self.critic_x, (tf.shape(self.critic_x)[0], 19, 16, 4))\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv1)\n",
    "            \n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 32,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_value = tf.contrib.layers.fully_connected(fc1, 1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_cost = tf.reduce_mean(tf.squared_difference(self.critic_value, self.critic_y))\n",
    "            self.critic_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.critic_cost)\n",
    "            \n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory_states[self.cnt] = state\n",
    "        self.memory_next_states[self.cnt] = next_state\n",
    "        self.memory_actions[self.cnt] = action\n",
    "        self.memory_rewards[self.cnt] = reward\n",
    "        self.cnt += 1\n",
    "        \n",
    "        if self.cnt == 2 * self.params.max_memory_size:\n",
    "            n = self.params.max_memory_size\n",
    "            self.memory_states[:n] = self.memory_states[-n:]\n",
    "            self.memory_next_states[:n] = self.memory_next_states[-n:]\n",
    "            self.memory_actions[:n] = self.memory_actions[-n:]\n",
    "            self.memory_rewards[:n] = self.memory_rewards[-n:]\n",
    "            self.cnt = n\n",
    "\n",
    "    def act(self, session, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(session, state, frame)\n",
    "    \n",
    "    def act_greedy(self, session, state, frame):\n",
    "        act_probs = session.run(self.actor_action_probs, {self.actor_x: [state]})[0]\n",
    "        return np.random.choice(np.arange(len(act_probs)), p=act_probs)\n",
    "\n",
    "    def think(self, session, batch_size, episode):\n",
    "        cnt = self.cnt\n",
    "        \n",
    "        values = session.run(self.critic_value, {self.critic_x: self.memory_states[:cnt]})\n",
    "        nextValues = session.run(self.critic_value, {self.critic_x: self.memory_next_states[:cnt]})\n",
    "        \n",
    "        criticY = self.memory_rewards[:cnt] + self.params.gamma * nextValues[:cnt]\n",
    "        actorY = criticY - values\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            states = self.memory_states[batch_indexes]\n",
    "            actorys = actorY[batch_indexes]\n",
    "            criticys = criticY[batch_indexes]\n",
    "            actions = self.memory_actions[batch_indexes]\n",
    "            \n",
    "            _ = session.run(self.actor_train_op,\n",
    "                            {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n",
    "            _ = session.run(self.critic_train_op,\n",
    "                            {self.critic_x: states, self.critic_y: criticys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (1216,), actions 6\n",
      "Run: 1\n",
      "episode: 1000/20000, reward -15.2, frames 1000, exploration rate: 0.86\n",
      "episode: 2000/20000, reward -18.2, frames 1000, exploration rate: 0.74\n",
      "episode: 3000/20000, reward -16.7, frames 1000, exploration rate: 0.64\n",
      "episode: 4000/20000, reward -18.3, frames 1000, exploration rate: 0.55\n",
      "episode: 5000/20000, reward -18.1, frames 1000, exploration rate: 0.47\n",
      "episode: 6000/20000, reward -17.5, frames 1000, exploration rate: 0.41\n",
      "episode: 7000/20000, reward -17.8, frames 1000, exploration rate: 0.35\n",
      "episode: 8000/20000, reward -18.1, frames 1000, exploration rate: 0.3\n",
      "episode: 9000/20000, reward -18.1, frames 1000, exploration rate: 0.26\n",
      "episode: 10000/20000, reward -18.0, frames 1000, exploration rate: 0.22\n",
      "episode: 11000/20000, reward -18.5, frames 1000, exploration rate: 0.19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3a0b6c957fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n\u001b[0;32m---> 28\u001b[0;31m                                               normalize_rewards=True)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(session, saver, env, agent, env_state_observer, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_state_observer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36menv_step\u001b[0;34m(self, env, action)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36mpreprocess_input_pong_v0\u001b[0;34m(I)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_input_pong_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m195\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mI\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m109\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "params = LearningParameters(env, env_state_observer.env_reset(env), episodes_count=20000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 1000)\n",
    "params.epsilon_min = 0.05\n",
    "params.max_memory_size = 20000\n",
    "params.pong_reset_discounted_reward = True\n",
    "agent = ActorCriticAgent(params)\n",
    "\n",
    "run_name += 1\n",
    "print('Run: ' + str(run_name))\n",
    "\n",
    "saver = TfSaver('logs')\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "config.operation_timeout_in_ms=60000\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# config = tf.ConfigProto()\n",
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.load_latest_checkpoint(session)\n",
    "    \n",
    "#     tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "    agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n",
    "                                              normalize_rewards=True)\n",
    "    # agent, rewards = train_reward_is_time(env, agent, params)\n",
    "    # agent, rewards = train(env, agent, params)\n",
    "    plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -9.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    evaluate(session, env, agent, env_state_observer, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOYAAAD8CAYAAABjJ9hGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b93699d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    show(session, env, agent, env_state_observer, params, 200, width=19, height=16, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/Pong ActorCriticAgent-9000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save rewards/model\n",
    "# pd.DataFrame(rewards).to_csv('models/rewards_40K_50K.csv', header=None)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, 'models/Pong ActorCriticAgent', global_step=9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
