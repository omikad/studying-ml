{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-30 08:42:40,703] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 6\n",
      "Raw observation space: (210, 160, 3)\n",
      "Max episode steps: 10000\n",
      "Preprocessed observation space: (1216,)\n",
      "Parameters: 1216\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CartPole-v1')\n",
    "# env_state_observer = EnvStateObserver(lambda x: x, concat_states_count=3)\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "env_state_observer = EnvStateObserver(preprocess_input_pong_v0, concat_states_count=4)\n",
    "sample_state = env_state_observer.env_reset(env)\n",
    "\n",
    "# env.render(close=True)\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print('Actions: {}'.format(env.action_space.n))\n",
    "print('Raw observation space: {}'.format(env.observation_space.shape))\n",
    "print('Max episode steps: {}'.format(env.spec.max_episode_steps))\n",
    "print('Preprocessed observation space: {}'.format(sample_state.shape))\n",
    "print('Parameters: {}'.format(np.prod(sample_state.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_next_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_actions = np.zeros((2 * self.params.max_memory_size))\n",
    "        self.memory_rewards = np.zeros((2 * self.params.max_memory_size, 1))\n",
    "        self.cnt = 0\n",
    "        self._build_actor_model()\n",
    "        self._build_critic_model()\n",
    "\n",
    "    def _build_actor_model(self):\n",
    "        with tf.variable_scope('actor'):\n",
    "            self.actor_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.actor_y = tf.placeholder(\"float\", [None, 1], name=\"Target\")\n",
    "            self.actor_action = tf.placeholder(tf.int32, [None,], name=\"Action\")\n",
    "\n",
    "            x = tf.reshape(self.actor_x, (tf.shape(self.actor_x)[0], 19, 16, 4))\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv1)\n",
    "            \n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 32,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            fc2 = tf.contrib.layers.fully_connected(fc1, self.params.action_size,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.actor_action_probs = tf.nn.softmax(fc2)\n",
    "\n",
    "            actions_ohe = tf.one_hot(self.actor_action, depth=self.params.action_size)\n",
    "            self.picked_action_probs = tf.reduce_sum(self.actor_action_probs * actions_ohe, axis=1, keep_dims=True)\n",
    "\n",
    "            self.actor_cost = tf.reduce_mean(-tf.log(1e-6 + self.picked_action_probs) * self.actor_y)\n",
    "            self.actor_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.actor_cost)\n",
    "\n",
    "    def _build_critic_model(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            self.critic_x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"State\")\n",
    "            self.critic_y = tf.placeholder(\"float\", [None, 1], name=\"Target\")\n",
    "\n",
    "            x = tf.reshape(self.critic_x, (tf.shape(self.critic_x)[0], 19, 16, 4))\n",
    "            conv1 = tf.contrib.layers.conv2d(x, 16, 4, 2, activation_fn=tf.nn.relu)\n",
    "            flattened = tf.contrib.layers.flatten(conv1)\n",
    "            \n",
    "            fc1 = tf.contrib.layers.fully_connected(flattened, 32,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_value = tf.contrib.layers.fully_connected(fc1, 1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.critic_cost = tf.reduce_mean(tf.squared_difference(self.critic_value, self.critic_y))\n",
    "            self.critic_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "                .minimize(self.critic_cost)\n",
    "            \n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory_states[self.cnt] = state\n",
    "        self.memory_next_states[self.cnt] = next_state\n",
    "        self.memory_actions[self.cnt] = action\n",
    "        self.memory_rewards[self.cnt] = reward\n",
    "        self.cnt += 1\n",
    "        \n",
    "        if self.cnt == 2 * self.params.max_memory_size:\n",
    "            n = self.params.max_memory_size\n",
    "            self.memory_states[:n] = self.memory_states[-n:]\n",
    "            self.memory_next_states[:n] = self.memory_next_states[-n:]\n",
    "            self.memory_actions[:n] = self.memory_actions[-n:]\n",
    "            self.memory_rewards[:n] = self.memory_rewards[-n:]\n",
    "            self.cnt = n\n",
    "\n",
    "    def act(self, session, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(session, state, frame)\n",
    "    \n",
    "    def act_greedy(self, session, state, frame):\n",
    "        act_probs = session.run(self.actor_action_probs, {self.actor_x: [state]})[0]\n",
    "        return np.random.choice(np.arange(len(act_probs)), p=act_probs)\n",
    "\n",
    "    def think(self, session, batch_size, episode):\n",
    "        cnt = self.cnt\n",
    "        \n",
    "        values = session.run(self.critic_value, {self.critic_x: self.memory_states[:cnt]})\n",
    "        nextValues = session.run(self.critic_value, {self.critic_x: self.memory_next_states[:cnt]})\n",
    "        \n",
    "        criticY = self.memory_rewards[:cnt] + self.params.gamma * nextValues[:cnt]\n",
    "        actorY = criticY - values\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            states = self.memory_states[batch_indexes]\n",
    "            actorys = actorY[batch_indexes]\n",
    "            criticys = criticY[batch_indexes]\n",
    "            actions = self.memory_actions[batch_indexes]\n",
    "            \n",
    "            _ = session.run(self.actor_train_op,\n",
    "                            {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n",
    "            _ = session.run(self.critic_train_op,\n",
    "                            {self.critic_x: states, self.critic_y: criticys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (1216,), actions 6\n",
      "Run: 1\n",
      "Loading model checkpoint logs/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from logs/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-30 08:43:08,052] Restoring parameters from logs/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 500/10000, reward -17.2, frames 1000, exploration rate: 0.86\n",
      "episode: 1000/10000, reward -16.7, frames 1000, exploration rate: 0.74\n",
      "episode: 1500/10000, reward -17.7, frames 1000, exploration rate: 0.64\n",
      "episode: 2000/10000, reward -16.8, frames 1000, exploration rate: 0.55\n",
      "episode: 2500/10000, reward -15.9, frames 1000, exploration rate: 0.47\n",
      "episode: 3000/10000, reward -17.7, frames 1000, exploration rate: 0.41\n",
      "episode: 3500/10000, reward -18.3, frames 1000, exploration rate: 0.35\n",
      "episode: 4000/10000, reward -16.8, frames 1000, exploration rate: 0.3\n",
      "episode: 4500/10000, reward -18.8, frames 1000, exploration rate: 0.26\n",
      "episode: 5000/10000, reward -18.3, frames 1000, exploration rate: 0.22\n",
      "episode: 5500/10000, reward -17.6, frames 1000, exploration rate: 0.19\n",
      "episode: 6000/10000, reward -18.0, frames 1000, exploration rate: 0.17\n",
      "episode: 6500/10000, reward -18.8, frames 1000, exploration rate: 0.14\n",
      "episode: 7000/10000, reward -18.4, frames 1000, exploration rate: 0.12\n",
      "episode: 7500/10000, reward -18.7, frames 1000, exploration rate: 0.11\n",
      "episode: 8000/10000, reward -18.2, frames 1000, exploration rate: 0.091\n",
      "episode: 8500/10000, reward -18.0, frames 1000, exploration rate: 0.078\n",
      "episode: 9000/10000, reward -19.1, frames 1000, exploration rate: 0.068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-26c4855937d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n\u001b[0;32m---> 28\u001b[0;31m                                               normalize_rewards=True)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(session, saver, env, agent, env_state_observer, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c8daa34d5822>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, session, batch_size, episode)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             _ = session.run(self.actor_train_op,\n\u001b[0;32m--> 100\u001b[0;31m                             {self.actor_x: states, self.actor_y: actorys, self.actor_action: actions})\n\u001b[0m\u001b[1;32m    101\u001b[0m             _ = session.run(self.critic_train_op,\n\u001b[1;32m    102\u001b[0m                             {self.critic_x: states, self.critic_y: criticys})\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m   def make_callable(self,\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mbuild_results\u001b[0;34m(self, session, tensor_values)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "params = LearningParameters(env, env_state_observer.env_reset(env), episodes_count=10000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 1000)\n",
    "params.epsilon_min = 0.05\n",
    "params.max_memory_size = 20000\n",
    "params.pong_reset_discounted_reward = True\n",
    "agent = ActorCriticAgent(params)\n",
    "\n",
    "run_name += 1\n",
    "print('Run: ' + str(run_name))\n",
    "\n",
    "saver = TfSaver('logs')\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "config.operation_timeout_in_ms=60000\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# config = tf.ConfigProto()\n",
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver.load_latest_checkpoint(session)\n",
    "    \n",
    "#     tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "    agent, rewards = train_discounted_rewards(session, saver, env, agent, env_state_observer, params,\n",
    "                                              normalize_rewards=True)\n",
    "    # agent, rewards = train_reward_is_time(env, agent, params)\n",
    "    # agent, rewards = train(env, agent, params)\n",
    "    plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -8.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    evaluate(session, env, agent, env_state_observer, params, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOYAAAD8CAYAAABjJ9hGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa0540b2090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session(config=config) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    show(session, env, agent, env_state_observer, params, 500, width=19, height=16, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/Pong ActorCriticAgent-9000'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save rewards/model\n",
    "# pd.DataFrame(rewards).to_csv('models/rewards_40K_50K.csv', header=None)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(session, 'models/Pong ActorCriticAgent', global_step=9000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
