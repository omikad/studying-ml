{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-13 13:18:34,814] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space 2, Actions: (4,), 4 parameters\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('MountainCar-v0')\n",
    "# env.my_preprocess_input = lambda x: x\n",
    "\n",
    "# env = gym.make('Breakout-v0')\n",
    "# env.my_preprocess_input = preprocess_input_breakout_v0\n",
    "\n",
    "# env = gym.make('Pong-v0')\n",
    "# env.my_preprocess_input = preprocess_input_pong_v0\n",
    "\n",
    "print('Observation space {}, Actions: {}, {} parameters'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnSiameseEmbeddingAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "\n",
    "        self.embedding_size = 3\n",
    "        self.hidden_size = 6\n",
    "        self.action_zeros = np.zeros(self.params.action_size)\n",
    "        self._build_oracle_model()\n",
    "        self._build_dqn_model()\n",
    "\n",
    "    def _build_dqn_model(self):\n",
    "        self.dqn_x = tf.placeholder(tf.float32, [None, self.embedding_size], name='dqn_x')\n",
    "        self.dqn_y = tf.placeholder(tf.float32, [None, self.params.action_size], name='dqn_y')\n",
    "        \n",
    "        fc1 = self._build_fully_connected_layer(self.dqn_x, self.hidden_size, 'dqn_fc1')\n",
    "        self.dqn_pred = self._build_fully_connected_layer(tf.nn.relu(fc1), self.params.action_size, 'dqn_fc2')\n",
    "        \n",
    "        cost = tf.nn.l2_loss(self.dqn_pred - self.dqn_y, name='dqn_cost')\n",
    "        self.dqn_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "            .minimize(cost)\n",
    "        \n",
    "    def _build_oracle_model(self):\n",
    "        input_size = self.params.state_size + self.params.action_size\n",
    "        self.oracle_x0 = tf.placeholder(tf.float32, [None, input_size], name='orc_x0')\n",
    "        self.oracle_x1 = tf.placeholder(tf.float32, [None, input_size], name='orc_x1')\n",
    "        self.oracle_y = tf.placeholder(tf.float32, [None, 1], name='orc_y')\n",
    "\n",
    "        with tf.variable_scope('seamese') as scope:\n",
    "            self.oracle_o1 = self._build_oracle_L(self.oracle_x0, self.hidden_size, self.embedding_size)\n",
    "            scope.reuse_variables()\n",
    "            self.oracle_o2 = self._build_oracle_L(self.oracle_x1, self.hidden_size, self.embedding_size)\n",
    "\n",
    "        distance = tf.reduce_sum(tf.pow(self.oracle_o1 - self.oracle_o2, 2), 1, keep_dims=True, name='distance')\n",
    "    \n",
    "        # if Y == 0 => min distance\n",
    "        # if Y == 1 => max distance\n",
    "        pos = (1 - self.oracle_y) * distance\n",
    "        neg = self.oracle_y * tf.maximum((5 - distance), 0)\n",
    "        self.oracle_loss = tf.reduce_mean(pos + neg, name='orc_loss')\n",
    "\n",
    "        self.oracle_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "            .minimize(self.oracle_loss)\n",
    "            \n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('oracle_loss', self.oracle_loss)\n",
    "            tf.summary.scalar('pos_avg_distance', tf.reduce_mean(pos))\n",
    "            tf.summary.scalar('neg_avg_distance', tf.reduce_mean(self.oracle_y * distance))\n",
    "        self.merged_summaries = tf.summary.merge_all()\n",
    "    \n",
    "    def _build_oracle_L(self, x, hidden_size, out_size):\n",
    "        fc1 = self._build_fully_connected_layer(x, hidden_size, 'fc1')\n",
    "        fc2 = self._build_fully_connected_layer(tf.nn.relu(fc1), out_size, 'fc2')\n",
    "        return fc2\n",
    "    \n",
    "    def _build_fully_connected_layer(self, bottom, size, name):\n",
    "        bottom_size = bottom.get_shape()[1]\n",
    "        initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "        w0 = tf.get_variable(name + 'w0', shape=[bottom_size, size], initializer=initer)\n",
    "        b0 = tf.get_variable(name + 'b0', initializer=tf.random_normal([size]))\n",
    "        fc = tf.add(tf.matmul(bottom, w0), b0)\n",
    "        return fc\n",
    "    \n",
    "    def get_embedding(self, state):\n",
    "        oracle_input = np.concatenate([state, self.action_zeros], axis=0)\n",
    "        embedding = session.run(self.oracle_o1, {self.oracle_x0: [oracle_input]})\n",
    "        return embedding\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        embedding = self.get_embedding(state)\n",
    "        act_values = session.run(self.dqn_pred, {self.dqn_x: embedding})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size, episode):\n",
    "#         if episode < 200:\n",
    "        self.think_oracle(batch_size, episode)\n",
    "        self.think_dqn(batch_size, episode)\n",
    "        \n",
    "    def think_dqn(self, batch_size, episode):\n",
    "        x = self.dqn_x\n",
    "        y = self.dqn_y\n",
    "        train_op = self.dqn_train_op\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.embedding_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            \n",
    "            embedding_state = self.get_embedding(state)\n",
    "            embedding_next_state = self.get_embedding(next_state)\n",
    "            \n",
    "            next_state_values = session.run(self.dqn_pred, {x: embedding_next_state})[0]\n",
    "            \n",
    "            target = session.run(self.dqn_pred, {x: embedding_state})[0]\n",
    "            target[action] = reward + self.params.gamma * np.amax(next_state_values)\n",
    "            \n",
    "            X[i], Y[i] = embedding_state[0], target\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            batch_x = X[batch_indexes]\n",
    "            batch_y = Y[batch_indexes]\n",
    "            _ = session.run(self.dqn_train_op, {x: batch_x, y: batch_y})\n",
    "            \n",
    "    def think_oracle(self, batch_size, episode):\n",
    "        x0 = self.oracle_x0\n",
    "        x1 = self.oracle_x1\n",
    "        y = self.oracle_y\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        rows = 2 * cnt - 4\n",
    "        positive_offsets = [1,2,3]\n",
    "        negative_offsets = [-1]\n",
    "        \n",
    "        X0 = np.zeros((rows, self.params.state_size + self.params.action_size))\n",
    "        X1 = np.zeros((rows, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((rows, 1))\n",
    "        action_ohe = np.zeros(self.params.action_size)\n",
    "        empty_inputs = [np.concatenate([self.memory[i][0], self.action_zeros], axis=0) for i in range(cnt)]\n",
    "        \n",
    "        for i in range(1, cnt - 1):\n",
    "            state, action, _, _ = self.memory[i]\n",
    "            action_ohe[action] = 1.0\n",
    "            input_x = np.concatenate([state, action_ohe], axis=0)\n",
    "            \n",
    "            pos_i = self._get_random_index(positive_offsets, cnt, i)\n",
    "            neg_i = self._get_random_index(negative_offsets, cnt, i)\n",
    "#             pos_i = i + 1\n",
    "#             neg_i = i - 1\n",
    "            \n",
    "            input_pos = empty_inputs[pos_i]\n",
    "            input_neg = empty_inputs[neg_i]\n",
    "            \n",
    "            ii = i * 2 - 2\n",
    "            X0[ii], X1[ii], Y[ii] = input_x, input_pos, 0\n",
    "            ii += 1\n",
    "            X0[ii], X1[ii], Y[ii] = input_x, input_neg, 1\n",
    "            \n",
    "            action_ohe[action] = 0.0\n",
    "\n",
    "#         for i in range(Y.shape[0]):\n",
    "#             if (X0[i,:].sum() == 0 or X1[i,:].sum() == 0):\n",
    "#                 print(cnt, rows)\n",
    "#                 print(X0.shape, X1.shape)\n",
    "#                 print i\n",
    "#                 print X0[i,:]\n",
    "#                 print X1[i,:]\n",
    "#                 raise Hi\n",
    "    \n",
    "        P = np.random.permutation(Y.shape[0])\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            batch_x0 = X0[batch_indexes]\n",
    "            batch_x1 = X1[batch_indexes]\n",
    "            batch_y = Y[batch_indexes]\n",
    "            _ = session.run(self.oracle_train_op, {x0: batch_x0, x1: batch_x1, y: batch_y})\n",
    "        \n",
    "        summary = session.run(self.merged_summaries, {x0: X0, x1: X1, y: Y})\n",
    "        tf_writer.add_summary(summary, episode)\n",
    "\n",
    "    def _get_random_index(self, offsets, cnt, i):\n",
    "        while True:\n",
    "            j = i + np.random.choice(offsets)\n",
    "            if j >= 0 and j < cnt:\n",
    "                return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (4,), actions 2\n",
      "Run: 16\n",
      "episode: 25/500, reward 22.4, frames 30, exploration rate: 0.87\n",
      "episode: 50/500, reward 20.0, frames 18, exploration rate: 0.75\n",
      "episode: 75/500, reward 15.6, frames 14, exploration rate: 0.65\n",
      "episode: 100/500, reward 14.0, frames 13, exploration rate: 0.56\n",
      "episode: 125/500, reward 13.7, frames 18, exploration rate: 0.48\n",
      "episode: 150/500, reward 12.4, frames 10, exploration rate: 0.41\n",
      "episode: 175/500, reward 10.9, frames 14, exploration rate: 0.35\n",
      "episode: 200/500, reward 10.9, frames 8, exploration rate: 0.31\n",
      "episode: 225/500, reward 11.4, frames 14, exploration rate: 0.26\n",
      "episode: 250/500, reward 11.8, frames 7, exploration rate: 0.23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-338a85408bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/studying-ml/RL/training_methods.pyc\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(env, agent, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-1ebb6a4b90a4>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, batch_size, episode)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m#         if episode < 200:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink_oracle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthink_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-1ebb6a4b90a4>\u001b[0m in \u001b[0;36mthink_dqn\u001b[0;34m(self, batch_size, episode)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0membedding_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0membedding_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-1ebb6a4b90a4>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0moracle_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_zeros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle_o1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle_x0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moracle_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0mfetch_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/excellent/ds/local/lib/python2.7/site-packages/tensorflow/python/util/compat.pyc\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \"\"\"\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env, episodes_count=500)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 10000)\n",
    "params.episodes_between_think = 1\n",
    "params.epsilon_min = 0.05\n",
    "params.earning_rate = 0.01\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "    tf_writer.close()\n",
    "tf.reset_default_graph()    \n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "session = tf.Session(config=config)\n",
    "run_name += 1\n",
    "print('Run: ' + str(run_name))\n",
    "\n",
    "agent = DqnSiameseEmbeddingAgent(params)\n",
    "\n",
    "tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
