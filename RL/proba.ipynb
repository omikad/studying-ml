{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from IPython import display\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "from training_methods import *\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-07 08:32:38,054] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space 2, Actions: (4,), 4 parameters\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.my_preprocess_input = lambda x: x\n",
    "\n",
    "print('Observation space {}, Actions: {}, {} parameters'.format(\n",
    "    env.action_space.n, env.observation_space.shape, np.prod(env.observation_space.shape)))\n",
    "\n",
    "run_name = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DqnSiameseEmbeddingAgent:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory = deque(maxlen=self.params.max_memory_size)\n",
    "        self.embedding_size = 10\n",
    "        self.action_zeros = np.zeros(self.params.action_size)\n",
    "        self._build_oracle_model()\n",
    "        self._build_dqn_model()\n",
    "\n",
    "    def _build_dqn_model(self):\n",
    "        self.dqn_x = tf.placeholder(tf.float32, [None, self.embedding_size], name='dqn_x')\n",
    "        self.dqn_y = tf.placeholder(tf.float32, [None, self.params.action_size], name='dqn_y')\n",
    "        \n",
    "        w0 = tf.Variable(tf.random_normal([self.embedding_size, 20]))\n",
    "        b0 = tf.Variable(tf.random_normal([20]))\n",
    "        w1 = tf.Variable(tf.random_normal([20, self.params.action_size]))\n",
    "        b1 = tf.Variable(tf.random_normal([self.params.action_size]))\n",
    "        \n",
    "        h0 = tf.add(tf.matmul(self.dqn_x, w0), b0)\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        \n",
    "        self.dqn_pred = tf.add(tf.matmul(h0, w1), b1)\n",
    "        \n",
    "        cost = tf.nn.l2_loss(self.dqn_pred - self.dqn_y, name='dqn_cost')\n",
    "        self.dqn_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "            .minimize(cost)\n",
    "        \n",
    "    def _build_oracle_model(self):\n",
    "        input_size = self.params.state_size + self.params.action_size\n",
    "        self.oracle_x0 = tf.placeholder(tf.float32, [None, input_size], name='orc_x0')\n",
    "        self.oracle_x1 = tf.placeholder(tf.float32, [None, input_size], name='orc_x1')\n",
    "        self.oracle_y = tf.placeholder(tf.float32, [None, 1], name='orc_y')\n",
    "\n",
    "        with tf.variable_scope('seamese') as scope:\n",
    "            self.oracle_o1 = self._build_oracle_L(self.oracle_x0, 20, self.embedding_size)\n",
    "            scope.reuse_variables()\n",
    "            self.oracle_o2 = self._build_oracle_L(self.oracle_x1, 20, self.embedding_size)\n",
    "\n",
    "        distance = tf.reduce_sum(tf.pow(self.oracle_o1 - self.oracle_o2, 2), 1, keep_dims=True, name='distance')\n",
    "\n",
    "        # if Y == 0 => min distance\n",
    "        # if Y == 1 => max distance\n",
    "        pos = (1 - self.oracle_y) * distance\n",
    "        neg = self.oracle_y * tf.maximum((1 - distance), 0)\n",
    "        self.oracle_loss = tf.reduce_sum(pos + neg, name='orc_loss')\n",
    "\n",
    "        self.oracle_train_op = tf.train.RMSPropOptimizer(learning_rate=self.params.learning_rate, decay=0.99) \\\n",
    "            .minimize(self.oracle_loss)\n",
    "            \n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('oracle_loss', self.oracle_loss)\n",
    "            tf.summary.scalar('pos_avg_distance', tf.reduce_mean(pos))\n",
    "            tf.summary.scalar('neg_avg_distance', tf.reduce_mean(self.oracle_y * distance))\n",
    "        self.merged_summaries = tf.summary.merge_all()\n",
    "    \n",
    "    def _build_oracle_L(self, x, hidden_size, out_size):\n",
    "        fc1 = self._build_fully_connected_layer(x, hidden_size, 'fc1')\n",
    "        fc2 = self._build_fully_connected_layer(tf.nn.relu(fc1), out_size, 'fc2')\n",
    "        return fc2\n",
    "    \n",
    "    def _build_fully_connected_layer(self, bottom, size, name):\n",
    "        bottom_size = bottom.get_shape()[1]\n",
    "        initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "        w0 = tf.get_variable(name + 'w0', shape=[bottom_size, size], initializer=initer)\n",
    "        b0 = tf.get_variable(name + 'b0', initializer=tf.random_normal([size]))\n",
    "        fc = tf.add(tf.matmul(bottom, w0), b0)\n",
    "        return fc\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory.append((state, action, reward, next_state))\n",
    "\n",
    "    def act(self, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(state, frame)\n",
    "    \n",
    "    def act_greedy(self, state, frame):\n",
    "        oracle_input = np.concatenate([state, self.action_zeros], axis=0)\n",
    "        embedding = session.run(self.oracle_o1, {self.oracle_x0: [oracle_input]})\n",
    "        act_values = session.run(self.dqn_pred, {self.dqn_x: embedding})[0]\n",
    "        return np.argmax(act_values)\n",
    "\n",
    "    def think(self, batch_size, episode):\n",
    "        self.think_oracle(batch_size, episode)\n",
    "        self.think_dqn(batch_size, episode)\n",
    "        \n",
    "    def think_dqn(self, batch_size, episode):\n",
    "        x = self.dqn_x\n",
    "        y = self.dqn_y\n",
    "        train_op = self.dqn_train_op\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X = np.zeros((cnt, self.embedding_size))\n",
    "        Y = np.zeros((cnt, self.params.action_size))\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            \n",
    "            oracle_input = np.concatenate([state, self.action_zeros], axis=0)\n",
    "            embedding_state = session.run(self.oracle_o1, {self.oracle_x0: [oracle_input]})\n",
    "            \n",
    "            oracle_input = np.concatenate([next_state, self.action_zeros], axis=0)\n",
    "            embedding_next_state = session.run(self.oracle_o1, {self.oracle_x0: [oracle_input]})\n",
    "            \n",
    "            target = session.run(self.dqn_pred, {x: embedding_state})[0]\n",
    "            target[action] = reward + self.params.gamma * \\\n",
    "                             np.amax(session.run(self.dqn_pred, {x: embedding_next_state})[0])\n",
    "            X[i], Y[i] = embedding_state[0], target\n",
    "\n",
    "        P = np.random.permutation(cnt)\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            batch_x = X[batch_indexes]\n",
    "            batch_y = Y[batch_indexes]\n",
    "            _ = session.run(self.dqn_train_op, {x: batch_x, y: batch_y})\n",
    "        \n",
    "    def think_oracle(self, batch_size, episode):\n",
    "        x0 = self.oracle_x0\n",
    "        x1 = self.oracle_x1\n",
    "        y = self.oracle_y\n",
    "        \n",
    "        cnt = len(self.memory)\n",
    "        X0 = np.zeros((2 * cnt, self.params.state_size + self.params.action_size))\n",
    "        X1 = np.zeros((2 * cnt, self.params.state_size + self.params.action_size))\n",
    "        Y = np.zeros((2 * cnt, 1))\n",
    "        action_ohe = np.zeros(self.params.action_size)\n",
    "        prev_state = np.concatenate([self.memory[np.random.randint(cnt)][0], self.action_zeros], axis=0)\n",
    "        for i in range(cnt):\n",
    "            state, action, reward, next_state = self.memory[i]\n",
    "            action_ohe[action] = 1.0\n",
    "            input_x = np.concatenate([state, action_ohe], axis=0)\n",
    "            input_pos = np.concatenate([next_state, self.action_zeros], axis=0)\n",
    "            input_neg = prev_state\n",
    "            j = i + cnt\n",
    "            X0[i], X1[i], Y[i] = input_x, input_pos, 0.0\n",
    "            X0[j], X1[j], Y[j] = input_x, input_neg, 1.0\n",
    "            action_ohe[action] = 0.0\n",
    "            prev_state = input_pos\n",
    "            \n",
    "        P = np.random.permutation(Y.shape[0])\n",
    "        for i in range(0, cnt, batch_size):\n",
    "            batch_indexes = P[i: i + batch_size]\n",
    "            batch_x0 = X0[batch_indexes]\n",
    "            batch_x1 = X1[batch_indexes]\n",
    "            batch_y = Y[batch_indexes]\n",
    "            _ = session.run(self.oracle_train_op, {x0: batch_x0, x1: batch_x1, y: batch_y})\n",
    "        \n",
    "        summary = session.run(self.merged_summaries, {x0: X0, x1: X1, y: Y})\n",
    "        tf_writer.add_summary(summary, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape (4,), actions 2\n",
      "episode: 50/1000, reward 22.7, frames 15, exploration rate: 0.87\n",
      "episode: 100/1000, reward 16.0, frames 11, exploration rate: 0.75\n",
      "episode: 150/1000, reward 19.6, frames 16, exploration rate: 0.64\n",
      "episode: 200/1000, reward 17.8, frames 16, exploration rate: 0.55\n",
      "episode: 250/1000, reward 11.2, frames 11, exploration rate: 0.48\n",
      "episode: 300/1000, reward 12.5, frames 8, exploration rate: 0.41\n",
      "episode: 350/1000, reward 11.8, frames 9, exploration rate: 0.35\n",
      "episode: 400/1000, reward 10.7, frames 9, exploration rate: 0.3\n",
      "episode: 450/1000, reward 11.7, frames 12, exploration rate: 0.26\n",
      "episode: 500/1000, reward 11.9, frames 10, exploration rate: 0.22\n",
      "episode: 550/1000, reward 10.9, frames 11, exploration rate: 0.19\n",
      "episode: 600/1000, reward 10.1, frames 10, exploration rate: 0.17\n",
      "episode: 650/1000, reward 9.7, frames 7, exploration rate: 0.14\n",
      "episode: 700/1000, reward 21.1, frames 16, exploration rate: 0.12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0b02c0ba7dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_discounted_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_rewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# agent, rewards = train_reward_is_time(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# agent, rewards = train(env, agent, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/studying-ml/RL/training_methods.py\u001b[0m in \u001b[0;36mtrain_discounted_rewards\u001b[0;34m(env, agent, params, normalize_rewards)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4e489905b6ff>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, batch_size, episode)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink_oracle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mthink_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4e489905b6ff>\u001b[0m in \u001b[0;36mthink_dqn\u001b[0;34m(self, batch_size, episode)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0moracle_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_zeros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0membedding_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle_o1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle_x0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moracle_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0moracle_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_zeros\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = LearningParameters(env, episodes_count=1000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 10000)\n",
    "params.episodes_between_think = 5\n",
    "params.epsilon_min = 0.05\n",
    "params.earning_rate = 0.01\n",
    "\n",
    "if 'session' in locals():\n",
    "    session.close()\n",
    "    tf_writer.close()\n",
    "tf.reset_default_graph()    \n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "session = tf.Session(config=config)\n",
    "run_name += 1\n",
    "\n",
    "agent = DqnSiameseEmbeddingAgent(params)\n",
    "\n",
    "tf_writer = tf.summary.FileWriter('logs/run' + str(run_name), session.graph)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "agent, rewards = train_discounted_rewards(env, agent, params, normalize_rewards=True)\n",
    "# agent, rewards = train_reward_is_time(env, agent, params)\n",
    "# agent, rewards = train(env, agent, params)\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, params, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
