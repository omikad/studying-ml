{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple, deque\n",
    "from IPython import display\n",
    "# pip install git+https://github.com/jakevdp/JSAnimation.git\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from scipy import stats\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Actions: 2\n",
      "Raw observation space: (4,)\n",
      "Max episode steps: 200\n",
      "((4,), 1.0)\n",
      "count    4.000000\n",
      "mean     0.026707\n",
      "std      0.174219\n",
      "min     -0.146667\n",
      "25%     -0.050359\n",
      "50%     -0.007467\n",
      "75%      0.069599\n",
      "max      0.268429\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102e9c650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PongV0Wrapper(gym.ObservationWrapper):\n",
    "    def preprocess_input_pong_v0(self, I):\n",
    "        I = I[35:195]\n",
    "        I[I == 144] = 0\n",
    "        I[I == 109] = 0\n",
    "        I = I[::2,::2,0] + I[1::2,::2,0] + I[::2,1::2,0] + I[1::2,1::2,0]\n",
    "        I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "        I = I[::2,::2] + I[1::2,::2] + I[::2,1::2] + I[1::2,1::2]\n",
    "        I[I != 0] = 1\n",
    "        I = I[0:19, 2:18]\n",
    "        return I.astype(np.float)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        return self.preprocess_input_pong_v0(observation)\n",
    "\n",
    "# env = PongV0Wrapper(gym.make('Pong-v0'))\n",
    "env = gym.make('CartPole-v0')\n",
    "    \n",
    "print('Actions: {}'.format(env.action_space.n))\n",
    "print('Raw observation space: {}'.format(env.observation_space.shape))\n",
    "print('Max episode steps: {}'.format(env.spec.max_episode_steps))\n",
    "\n",
    "def test(env):\n",
    "    state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    print(state.shape, reward)\n",
    "    print(pd.Series(state.flatten()).describe())\n",
    "    plt.imshow(state.reshape((1, 4)), cmap='Greys')\n",
    "\n",
    "test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningParameters:\n",
    "    def __init__(self, env, state, episodes_count):\n",
    "        self.state_shape = state.shape\n",
    "        self.state_size = np.prod(self.state_shape)\n",
    "        self.action_size = env.action_space.n\n",
    "        self.episodes_count = episodes_count\n",
    "        self.max_frame_in_episode = env.spec.max_episode_steps\n",
    "        self.max_memory_size = 100000\n",
    "        self.episodes_between_think = 1\n",
    "        self.episodes_warmup = 5\n",
    "\n",
    "        self.gamma = 0.95                # rewards discount rate\n",
    "        self.epsilon = 1.0               # exploration rate\n",
    "        self.epsilon_start = self.epsilon\n",
    "        self.epsilon_min = 0.0001        # min exploration rate\n",
    "        self.learning_rate = 0.1         # learning rate for algorithm\n",
    "        self.learning_rate_model = 0.01  # learning rate for model\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        self.reward_is_time = False\n",
    "        self.discount_rewards = True\n",
    "        self.normalize_rewards = True\n",
    "\n",
    "def decay_exploration_rate(params, episode):\n",
    "    # Exponential rate decay\n",
    "    # y(0) = start\n",
    "    # y(1) = start * x\n",
    "    # y(2) = start * x^2\n",
    "    # y(steps) = start * x^steps = min => x = (min/start) ^ (1/steps)\n",
    "    # y(t) = start * x^t\n",
    "    x = math.pow(params.epsilon_min / params.epsilon_start, 1.0 / params.episodes_count)\n",
    "    params.epsilon = params.epsilon_start * math.pow(x, episode)\n",
    "\n",
    "def set_rewards_time(episode_rewards):\n",
    "    for i in range(len(episode_rewards)):\n",
    "        episode_rewards[i] = len(episode_rewards) - i\n",
    "\n",
    "def set_discount_rewards(episode_rewards):\n",
    "    discounted_reward = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        reward = episode_rewards[i]\n",
    "        if params.pong_reset_discounted_reward and reward != 0:\n",
    "            discounted_reward = 0.0\n",
    "        discounted_reward = reward + discounted_reward * params.gamma\n",
    "        episode_rewards[i] = discounted_reward\n",
    "        \n",
    "def normalize(arr):\n",
    "    arr -= np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    if std != 0:\n",
    "        arr /= std\n",
    "    return arr\n",
    "        \n",
    "def train(session, tfwriter, env, agent, params):\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(params.episodes_count):\n",
    "        state = env.reset()\n",
    "\n",
    "        replays = []\n",
    "\n",
    "        for frame in range(params.max_frame_in_episode):\n",
    "            action = agent.act(session, state, frame)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            replays.append((frame, state, action, reward, next_state))\n",
    "            state = next_state\n",
    "\n",
    "        episode_rewards = list(r[3] for r in replays)\n",
    "        rewards.append(sum(episode_rewards))\n",
    "        \n",
    "        if params.reward_is_time:\n",
    "            set_rewards_time(episode_rewards)\n",
    "        if params.discount_rewards:\n",
    "            set_discount_rewards(episode_rewards)\n",
    "        if params.normalize_rewards:\n",
    "            episode_rewards = normalize(episode_rewards)\n",
    "\n",
    "        for i in range(len(replays)):\n",
    "            frame, state, action, _, next_state = replays[i]\n",
    "            agent.remember(state, action, episode_rewards[i], next_state, frame)\n",
    "\n",
    "        tfwriter.add_summary(tf.Summary(value=[\n",
    "                tf.Summary.Value(tag='Summaries/Episode_reward', simple_value=rewards[-1]),\n",
    "                tf.Summary.Value(tag='Summaries/Episode_frames', simple_value=len(replays)),\n",
    "                tf.Summary.Value(tag='Summaries/Epsilon', simple_value=params.epsilon),\n",
    "            ]), episode)\n",
    "\n",
    "        if episode > params.episodes_warmup and (episode + 1) % params.episodes_between_think == 0:\n",
    "            agent.think(session, tfwriter, params.batch_size, episode)\n",
    "\n",
    "        decay_exploration_rate(params, episode)\n",
    "\n",
    "    return agent, rewards\n",
    "\n",
    "# https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: state, action OHE\n",
    "# Output: next state, Q reward on state\n",
    "# Train: (state, action, time delta) ~ (next state after time delta passed, reward)\n",
    "class DqnAgentPredictDistantFuture:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.memory_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_next_states = np.zeros((2 * self.params.max_memory_size, self.params.state_size))\n",
    "        self.memory_actions = np.zeros((2 * self.params.max_memory_size), dtype=np.int32)\n",
    "        self.memory_rewards = np.zeros((2 * self.params.max_memory_size))\n",
    "        self.cnt = 0\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.x = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_x\")\n",
    "        self.xnext = tf.placeholder(\"float\", [None, self.params.state_size], name=\"Placeholder_xnext\")\n",
    "        self.a = tf.placeholder(\"float\", [None, self.params.action_size], name=\"Placeholder_a\")\n",
    "        self.y = tf.placeholder(\"float\", [None], name=\"Placeholder_y\")\n",
    "        \n",
    "        print('actions_ohe:', self.a)\n",
    "\n",
    "        inp = tf.concat([self.x, self.a], axis=1)\n",
    "        print('inp', inp)\n",
    "        \n",
    "        inner = tf.contrib.layers.fully_connected(\n",
    "            inp,\n",
    "            12,\n",
    "            activation_fn=tf.nn.relu,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        print('inner', inner)\n",
    "        \n",
    "        self.prednext = tf.contrib.layers.fully_connected(\n",
    "            inputs=inner,\n",
    "            num_outputs=self.params.state_size,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        print('self.prednext', self.prednext)\n",
    "        \n",
    "        self.pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=inner,\n",
    "            num_outputs=1,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        print('self.pred', self.pred)\n",
    "        \n",
    "        cost_y = tf.reduce_mean(tf.squared_difference(self.pred, self.y))\n",
    "        cost_state = tf.reduce_mean(tf.squared_difference(self.prednext, self.xnext))\n",
    "        \n",
    "        self.cost = cost_y + cost_state\n",
    "        print('self.cost', self.cost)\n",
    "\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(learning_rate=self.params.learning_rate) \\\n",
    "            .minimize(self.cost)\n",
    "            \n",
    "        tf.summary.scalar('Summaries/Cost_QValue', cost_y)\n",
    "        tf.summary.scalar('Summaries/Cost_Predict_State', cost_state)\n",
    "        tf.summary.scalar('Summaries/Total_cost', self.cost)\n",
    "        self.merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, frame):\n",
    "        self.memory_states[self.cnt] = state\n",
    "        self.memory_next_states[self.cnt] = next_state\n",
    "        self.memory_actions[self.cnt] = action\n",
    "        self.memory_rewards[self.cnt] = reward\n",
    "        self.cnt += 1\n",
    "        \n",
    "        if self.cnt == 2 * self.params.max_memory_size:\n",
    "            n = self.params.max_memory_size\n",
    "            self.memory_states[:n] = self.memory_states[-n:]\n",
    "            self.memory_next_states[:n] = self.memory_next_states[-n:]\n",
    "            self.memory_actions[:n] = self.memory_actions[-n:]\n",
    "            self.memory_rewards[:n] = self.memory_rewards[-n:]\n",
    "            self.cnt = n\n",
    "\n",
    "    def act(self, session, state, frame):\n",
    "        if np.random.rand() <= self.params.epsilon:\n",
    "            return np.random.randint(0, self.params.action_size)\n",
    "        return self.act_greedy(session, state, frame)\n",
    "    \n",
    "    def act_greedy(self, session, state, frame):\n",
    "        act_values = session.run(self.pred, {\n",
    "            self.x: [state] * 6,\n",
    "            self.a: [\n",
    "                [1., 0.],\n",
    "                [0., 1.],\n",
    "                [2., 0.],\n",
    "                [0., 2.],\n",
    "                [2., 1.],\n",
    "                [1., 2.],\n",
    "            ]})\n",
    "        return np.argmax(act_values) % 2\n",
    "    \n",
    "    def think(self, session, tfwriter, batch_size, episode):\n",
    "        cnt = self.cnt\n",
    "        \n",
    "        action0 = np.zeros((cnt, 2))\n",
    "        action0[:,0] = 1.\n",
    "        action1 = np.zeros((cnt, 2))\n",
    "        action1[:,1] = 1.\n",
    "        \n",
    "        # TODO: Update Q value with look forward\n",
    "        nextValues = np.zeros((cnt, self.params.action_size))\n",
    "        action = np.zeros((cnt, self.params.action_size))\n",
    "        for a in range(self.params.action_size):\n",
    "            action[:,a] = 1.\n",
    "            nextValues[:,a] = session.run(self.pred, {self.x: self.memory_next_states[:cnt], self.a: action})[:,0]\n",
    "            action[:,a] = 0.\n",
    "\n",
    "        # Reward is time:\n",
    "        qrewards = self.memory_rewards[:cnt] / self.params.max_frame_in_episode \\\n",
    "            + self.params.gamma * np.amax(nextValues, axis=1)\n",
    "#         qrewards = self.memory_rewards[:cnt] + self.params.gamma * np.amax(nextValues, axis=1)\n",
    "\n",
    "        X = np.concatenate([\n",
    "            self.memory_states[:cnt],\n",
    "            self.memory_states[:cnt-1],\n",
    "            self.memory_states[:cnt-2]], axis=0)\n",
    "        \n",
    "        action_len_1 = np.zeros((cnt, self.params.action_size))\n",
    "        action_len_1[np.arange(cnt), self.memory_actions[:cnt]] = 1.0\n",
    "        \n",
    "        action_len_2 = np.zeros((cnt-1, self.params.action_size))\n",
    "        action_len_2[np.arange(cnt-1), self.memory_actions[:cnt-1]] = 1.0\n",
    "        action_len_2[np.arange(cnt-1), self.memory_actions[1:cnt]] += 1.0\n",
    "        \n",
    "        action_len_3 = np.zeros((cnt-2, self.params.action_size))\n",
    "        action_len_3[np.arange(cnt-2), self.memory_actions[:cnt-2]] = 1.0\n",
    "        action_len_3[np.arange(cnt-2), self.memory_actions[1:cnt-1]] += 1.0\n",
    "        action_len_3[np.arange(cnt-2), self.memory_actions[2:cnt]] += 1.0\n",
    "        \n",
    "        A = np.concatenate([\n",
    "            action_len_1,\n",
    "            action_len_2,\n",
    "            action_len_3], axis=0)\n",
    "        \n",
    "        XN = np.concatenate([\n",
    "            self.memory_next_states[:cnt],\n",
    "            self.memory_next_states[1:cnt],\n",
    "            self.memory_next_states[2:cnt]], axis=0)\n",
    "        \n",
    "        Y = np.concatenate([\n",
    "            qrewards,\n",
    "            qrewards[1:cnt],\n",
    "            qrewards[2:cnt]], axis=0)\n",
    "        \n",
    "        # Normalize rewards here, not in train\n",
    "#         Y = normalize(Y)\n",
    "        \n",
    "        cost_before = session.run(self.cost, {\n",
    "            self.x: X,\n",
    "            self.xnext: XN,\n",
    "            self.a: A,\n",
    "            self.y: Y})\n",
    "        \n",
    "        cost_after = cost_before\n",
    "        for epoch in range(20):\n",
    "\n",
    "            P = np.random.permutation(cnt)\n",
    "            for i in range(0, len(P), batch_size):\n",
    "                batch_indexes = P[i: i + batch_size]\n",
    "                batch_x = X[batch_indexes, :]\n",
    "                batch_xnext = XN[batch_indexes, :]\n",
    "                batch_a = A[batch_indexes]\n",
    "                batch_y = Y[batch_indexes]\n",
    "                _ = session.run(self.train_op, {\n",
    "                    self.x: batch_x,\n",
    "                    self.xnext: batch_xnext,\n",
    "                    self.a: batch_a,\n",
    "                    self.y: batch_y})\n",
    "\n",
    "            summary, cost_curr = session.run([self.merged_summaries, self.cost], {\n",
    "                self.x: X,\n",
    "                self.xnext: XN,\n",
    "                self.a: A,\n",
    "                self.y: Y})\n",
    "\n",
    "            if cost_curr > cost_after:  # early stopping\n",
    "                break\n",
    "\n",
    "            cost_after = cost_curr\n",
    "\n",
    "        tfwriter.add_summary(summary, episode)\n",
    "        tfwriter.add_summary(tf.Summary(value=[\n",
    "                tf.Summary.Value(tag='Summaries/Dataset_len', simple_value=X.shape[0]),\n",
    "                tf.Summary.Value(tag='Summaries/Train_sub_epochs', simple_value=epoch),\n",
    "                tf.Summary.Value(tag='DataStats/Y_min', simple_value=np.min(Y)),\n",
    "                tf.Summary.Value(tag='DataStats/Y_max', simple_value=np.max(Y)),\n",
    "                tf.Summary.Value(tag='DataStats/Rewards_min', simple_value=np.min(self.memory_rewards[:cnt])),\n",
    "                tf.Summary.Value(tag='DataStats/Rewards_max', simple_value=np.max(self.memory_rewards[:cnt])),\n",
    "            ]), episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('actions_ohe:', <tf.Tensor 'Placeholder_a:0' shape=(?, 2) dtype=float32>)\n",
      "('inp', <tf.Tensor 'concat:0' shape=(?, 6) dtype=float32>)\n",
      "('inner', <tf.Tensor 'fully_connected/Relu:0' shape=(?, 12) dtype=float32>)\n",
      "('self.prednext', <tf.Tensor 'fully_connected_1/BiasAdd:0' shape=(?, 4) dtype=float32>)\n",
      "('self.pred', <tf.Tensor 'fully_connected_2/BiasAdd:0' shape=(?, 1) dtype=float32>)\n",
      "('self.cost', <tf.Tensor 'add:0' shape=() dtype=float32>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-aa3f11ab67c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"run += 1\\ntf.reset_default_graph()\\n\\nparams = LearningParameters(env, env.reset(), episodes_count=1000)\\nparams.max_frame_in_episode = min(params.max_frame_in_episode, 500)\\nparams.learning_rate = 0.003\\nparams.epsilon_min = 0.05\\nparams.max_memory_size = 2000\\nparams.pong_reset_discounted_reward = False\\nparams.reward_is_time = True\\nparams.discount_rewards = False\\nparams.normalize_rewards = False\\nagent = DqnAgentPredictDistantFuture(params)\\n\\n# Train on GPU\\nconfig = tf.ConfigProto(log_device_placement=True)\\n# config.gpu_options.per_process_gpu_memory_fraction=0.7\\n# config.operation_timeout_in_ms=60000\\n\\n# Train on CPU\\n# config = tf.ConfigProto(device_count = {'GPU': 0})\\n\\nwith tf.Session(config=config) as session:\\n    tfwriter = tf.summary.FileWriter('logs/run' + str(run), session.graph)\\n\\n    session.run(tf.global_variables_initializer())\\n\\n    agent, rewards = train(session, tfwriter, env, agent, params)\\n\\n    tfwriter.close()\\n    \\n    plt.plot(rewards)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-ec6e2a63e5b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(session, tfwriter, env, agent, params)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_warmup\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes_between_think\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdecay_exploration_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-a190ad38e1bf>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, session, tfwriter, batch_size, episode)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxnext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mXN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             self.y: Y})\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mcost_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/spastukhov/Documents/ds/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run += 1\n",
    "tf.reset_default_graph()\n",
    "\n",
    "params = LearningParameters(env, env.reset(), episodes_count=1000)\n",
    "params.max_frame_in_episode = min(params.max_frame_in_episode, 500)\n",
    "params.learning_rate = 0.003\n",
    "params.epsilon_min = 0.05\n",
    "params.max_memory_size = 2000\n",
    "params.pong_reset_discounted_reward = False\n",
    "params.reward_is_time = True\n",
    "params.discount_rewards = False\n",
    "params.normalize_rewards = False\n",
    "agent = DqnAgentPredictDistantFuture(params)\n",
    "\n",
    "# Train on GPU\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "# config.gpu_options.per_process_gpu_memory_fraction=0.7\n",
    "# config.operation_timeout_in_ms=60000\n",
    "\n",
    "# Train on CPU\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "\n",
    "with tf.Session(config=config) as session:\n",
    "    tfwriter = tf.summary.FileWriter('logs/run' + str(run), session.graph)\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    agent, rewards = train(session, tfwriter, env, agent, params)\n",
    "\n",
    "    tfwriter.close()\n",
    "    \n",
    "    plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
