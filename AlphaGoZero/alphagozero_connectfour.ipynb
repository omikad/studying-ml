{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Env based on: https://pettingzoo.farama.org/environments/classic/connect_four/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter, deque\n",
    "import torch\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import connect_four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySummaryWriter(SummaryWriter):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.points_cnt = Counter()\n",
    "\n",
    "    def append_scalar(self, name, value):\n",
    "        step = self.points_cnt[name]\n",
    "        self.points_cnt[name] += 1\n",
    "        self.add_scalar(name, value, step)\n",
    "\n",
    "\n",
    "TENSORBOARD = MySummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 3 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 3 (-1, True, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      ".......\n",
      "....0..\n",
      "....0.0\n",
      "101.0.0\n",
      "0111101\n",
      "0111001\n"
     ]
    }
   ],
   "source": [
    "def show():\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    state, mask = env.reset()\n",
    "\n",
    "    for step in range(50):\n",
    "        I = np.where(mask == 1)[0]\n",
    "        action = np.random.choice(I)\n",
    "\n",
    "        state, mask, reward, done = env.step(action)\n",
    "\n",
    "        print(step % 2, action, (reward, done, state.board.shape, mask))\n",
    "\n",
    "        if done:\n",
    "            env.render_ascii()\n",
    "            break\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomA vs RandomB:\n",
      "   RandomA  first turn wins: 278\n",
      "   RandomA second turn wins: 199\n",
      "   RandomB  first turn wins: 298\n",
      "   RandomB second turn wins: 219\n",
      "                      draws: 6\n",
      "Random vs AlwaysLeft:\n",
      "   Random  first turn wins: 131\n",
      "   Random second turn wins: 69\n",
      "   AlwaysLeft  first turn wins: 431\n",
      "   AlwaysLeft second turn wins: 369\n",
      "                      draws: 0\n",
      "Random vs AlwaysRight:\n",
      "   Random  first turn wins: 115\n",
      "   Random second turn wins: 85\n",
      "   AlwaysRight  first turn wins: 415\n",
      "   AlwaysRight second turn wins: 385\n",
      "                      draws: 0\n",
      "AlwaysLeft vs AlwaysRight:\n",
      "   AlwaysLeft  first turn wins: 500\n",
      "   AlwaysLeft second turn wins: 0\n",
      "   AlwaysRight  first turn wins: 500\n",
      "   AlwaysRight second turn wins: 0\n",
      "                      draws: 0\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        action = np.random.choice(I)\n",
    "        return action\n",
    "\n",
    "\n",
    "class AlwaysLeftAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        return I[0]\n",
    "\n",
    "\n",
    "class AlwaysRightAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        return I[-1]\n",
    "\n",
    "\n",
    "def battle(env, agent0, agent1, n_games, n_max_steps):\n",
    "    \"\"\"\n",
    "    Play `n_games` where each game is at most `n_max_steps` turns.\n",
    "    Return counter of game results as array `[\n",
    "        agent0  first turn and agent0 wins,\n",
    "        agent0 second turn and agent0 wins,\n",
    "        agent1  first turn and agent1 wins,\n",
    "        agent1 second turn and agent1 wins,\n",
    "        draws\n",
    "    ]`\n",
    "    \"\"\"\n",
    "\n",
    "    results = [0, 0, 0, 0, 0]\n",
    "    agents = [agent0, agent1]\n",
    "\n",
    "    for game in range(n_games):\n",
    "        state, mask = env.reset()\n",
    "\n",
    "        agent_id_first_turn = game % 2  # Switch sides every other game\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "            action = agents[(agent_id_first_turn + step) % 2].get_action(state, mask)\n",
    "\n",
    "            state, mask, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                if reward == 0:   # draw\n",
    "                    results[4] += 1\n",
    "                else:\n",
    "                    if agent_id_first_turn == 0:   # agent0 first turn, agent1 second turn\n",
    "                        if reward == 1:\n",
    "                            results[0] += 1\n",
    "                        else:\n",
    "                            results[3] += 1\n",
    "\n",
    "                    else:  # agent0 second turn, agent1 first turn\n",
    "                        if reward == 1:\n",
    "                            results[2] += 1\n",
    "                        else:\n",
    "                            results[1] += 1\n",
    "\n",
    "                break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def show_sample_agents_battle():\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    for name0, agent0, name1, agent1 in [\n",
    "        (\"RandomA\", RandomAgent(), \"RandomB\", RandomAgent()),\n",
    "        (\"Random\", RandomAgent(), \"AlwaysLeft\", AlwaysLeftAgent()),\n",
    "        (\"Random\", RandomAgent(), \"AlwaysRight\", AlwaysRightAgent()),\n",
    "        (\"AlwaysLeft\", AlwaysLeftAgent(), \"AlwaysRight\", AlwaysRightAgent()),\n",
    "    ]:\n",
    "        battle_results = battle(env, agent0, agent1, n_games=1000, n_max_steps=50)\n",
    "        print(f\"{name0} vs {name1}:\")\n",
    "        print(f\"   {name0}  first turn wins: {battle_results[0]}\")\n",
    "        print(f\"   {name0} second turn wins: {battle_results[1]}\")\n",
    "        print(f\"   {name1}  first turn wins: {battle_results[2]}\")\n",
    "        print(f\"   {name1} second turn wins: {battle_results[3]}\")\n",
    "        print(f\"                      draws: {battle_results[4]}\")\n",
    "\n",
    "show_sample_agents_battle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo Zero plays Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayEpisode:\n",
    "    def __init__(self, first_state):\n",
    "        self.states = [first_state]\n",
    "        self.actions = []\n",
    "        self.terminal_rewards = [0, 0]\n",
    "\n",
    "    def on_action(self, action: int, reward: int, done: bool, next_state: connect_four.HashableState):\n",
    "        if done:\n",
    "            player_idx = self.states[-1].player_idx\n",
    "            self.terminal_rewards[player_idx] = reward\n",
    "            self.terminal_rewards[1 - player_idx] = -reward\n",
    "        self.actions.append(action)\n",
    "        self.states.append(next_state)\n",
    "\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, max_episodes):\n",
    "        self.episodes = deque()\n",
    "        self.max_episodes = max_episodes\n",
    "\n",
    "    def on_reset(self, state: connect_four.HashableState):\n",
    "        self.episodes.append(ExperienceReplayEpisode(state))\n",
    "        while len(self.episodes) > self.max_episodes:\n",
    "            self.episodes.popleft()\n",
    "\n",
    "    def on_action(self, action: int, reward: int, done: bool, next_state: connect_four.HashableState):\n",
    "        self.episodes[-1].on_action(action, reward, done, next_state)\n",
    "\n",
    "    def clear(self):\n",
    "        self.episodes.clear()\n",
    "\n",
    "    def yield_training_tuples(self):\n",
    "        for episode in self.episodes:\n",
    "            for i in range(len(episode.actions)):\n",
    "                state = episode.states[i]\n",
    "                action = episode.actions[i]\n",
    "                reward = episode.terminal_rewards[state.player_idx]\n",
    "\n",
    "                yield state, action, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZeroModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement AlphaGo Zero model with two heads: for action probabilities and state value\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_common = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),   # (6, 7, 2) -> 84\n",
    "            torch.nn.Linear(84, 32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.model_action_logits = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 7),\n",
    "        )\n",
    "\n",
    "        self.model_state_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def _rotate_board(self, state: connect_four.HashableState):\n",
    "        \"\"\"\n",
    "        Make current player to have coins at `board[:, :, 0]`, and opponent at `board[:, :, 1]`\n",
    "        \"\"\"\n",
    "        board = state.board\n",
    "        if state.player_idx == 1:\n",
    "            board = np.flip(board, axis=2).copy()\n",
    "        return board\n",
    "\n",
    "\n",
    "    def forward(self, board):\n",
    "        B = board.shape[0]\n",
    "        assert board.shape == (B, 6, 7, 2)\n",
    "\n",
    "        common = self.model_common(board)\n",
    "        action_logits = self.model_action_logits(common)\n",
    "        state_value = self.model_state_value(common)\n",
    "\n",
    "        # action_logits.shape == (B, 7)\n",
    "        # state_value.shape == (B, 1)\n",
    "        return action_logits, state_value\n",
    "\n",
    "\n",
    "    def get_p_v_single_state(self, state: connect_four.HashableState, need_p: bool, need_v: bool):\n",
    "        board = self._rotate_board(state)\n",
    "        board = torch.from_numpy(board.reshape((1, 6, 7, 2))).float()\n",
    "        common = self.model_common(board)\n",
    "\n",
    "        action_logits, state_value = None, None\n",
    "\n",
    "        if need_p:\n",
    "            action_logits = self.model_action_logits(common)\n",
    "            action_probs = torch.nn.functional.softmax(action_logits, dim=1)\n",
    "            action_probs = action_probs.detach().cpu().numpy()[0, :]\n",
    "\n",
    "        if need_v:\n",
    "            state_value = self.model_state_value(common)\n",
    "            state_value = state_value.detach().cpu().numpy()[0, 0]\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "    def get_rollout_action(self, state: connect_four.HashableState, actions_mask):\n",
    "        assert actions_mask.shape == (7,)\n",
    "\n",
    "        action_probs, _ = self.get_p_v_single_state(state, need_p=True, need_v=False)\n",
    "        action_probs[actions_mask == 0] = -np.inf\n",
    "\n",
    "        return np.argmax(action_probs)\n",
    "\n",
    "    def go_train(self, tree, experience_replay, optimizer, batch_size):\n",
    "        self.train()\n",
    "\n",
    "        data = list(experience_replay.yield_training_tuples())\n",
    "\n",
    "        TENSORBOARD.append_scalar('data len', len(data))\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "        for bi in range(batch_size, len(data) + 1, batch_size):\n",
    "            boards = []\n",
    "            actual_probs = []\n",
    "            actual_values = []\n",
    "\n",
    "            for state, action, reward in data[bi - batch_size : bi]:\n",
    "                boards.append(self._rotate_board(state))\n",
    "\n",
    "                cnt_visits = tree.N_cnt_visits[state]\n",
    "                actual_probs.append(cnt_visits / np.sum(cnt_visits))\n",
    "\n",
    "                actual_values.append(reward)\n",
    "\n",
    "            boards = torch.as_tensor(boards).float()\n",
    "            # print(f\"[go_train] boards {boards.shape} {boards.dtype}\")   # (B, 7, 8, 2)\n",
    "\n",
    "            actual_probs = torch.as_tensor(actual_probs).float()\n",
    "            # print(f\"[go_train] actual_probs {actual_probs.shape} {actual_probs.dtype}\")  # (B, 7)\n",
    "\n",
    "            actual_values = torch.as_tensor(actual_values).view((batch_size, 1)).float()\n",
    "            # print(f\"[go_train] actual_values {actual_values.shape} {actual_values.dtype}\")  # (B, 1)\n",
    "\n",
    "            pred_action_logits, pred_state_value = self.forward(boards)\n",
    "            # print(f\"[go_train] pred_action_logits {pred_action_logits.shape} {pred_action_logits.dtype}\")  # (B, 7)\n",
    "            # print(f\"[go_train] pred_state_value {pred_state_value.shape} {pred_state_value.dtype}\")  # (B, 1)\n",
    "\n",
    "            states_loss = torch.nn.functional.mse_loss(actual_values, pred_state_value)\n",
    "            action_loss = torch.nn.functional.cross_entropy(pred_action_logits, actual_probs)\n",
    "\n",
    "            TENSORBOARD.append_scalar('states_loss', states_loss.item())\n",
    "            TENSORBOARD.append_scalar('action_loss', action_loss.item())\n",
    "\n",
    "            loss = states_loss + action_loss\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "class AlphaGoZeroAgent:\n",
    "    \"\"\"\n",
    "    Trained agent which can play Connect Four game using `AlphaGoZeroModel` model\n",
    "    \"\"\"\n",
    "    def __init__(self, model: AlphaGoZeroModel) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state: connect_four.HashableState, actions_mask):\n",
    "        self.model.eval()\n",
    "        return self.model.get_rollout_action(state, actions_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Tree:\n",
    "    \"\"\"\n",
    "    Tree structure which keeps various statistics needed for MCTS\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # Structure of all dicts: { state -> [ action -> value ] }\n",
    "        self.N_cnt_visits = dict()\n",
    "        self.W_sum_vals = dict()\n",
    "        self.Q_values = dict()\n",
    "        self.P_action_probs = dict()\n",
    "\n",
    "        self.V_state_values = dict()  # { state -> predicted state value v(s) }\n",
    "\n",
    "    def get_action(self, state: connect_four.HashableState):\n",
    "        N_s_a = self.N_cnt_visits[state]\n",
    "        Q_s_a = self.Q_values[state]\n",
    "        P_s_a = self.P_action_probs[state]\n",
    "        U_s_a = 4 * P_s_a * np.sqrt(np.sum(N_s_a)) / (1 + N_s_a)\n",
    "        action = np.argmax(Q_s_a + U_s_a)\n",
    "        return action\n",
    "\n",
    "    def add_node(self, model: AlphaGoZeroModel, state, actions_mask):\n",
    "        action_probs, state_value = model.get_p_v_single_state(state, need_p=True, need_v=True)\n",
    "        action_probs[actions_mask == 0] = 0\n",
    "        action_probs /= np.sum(action_probs)\n",
    "\n",
    "        actions_len = len(actions_mask)\n",
    "        self.N_cnt_visits[state] = np.zeros(actions_len)\n",
    "        self.W_sum_vals[state] = np.zeros(actions_len)\n",
    "        self.Q_values[state] = np.zeros(actions_len)\n",
    "        self.Q_values[state][actions_mask == 0] = -np.inf\n",
    "        self.P_action_probs[state] = action_probs\n",
    "        self.V_state_values[state] = state_value\n",
    "\n",
    "    def backpropagate(self, sa_array, reward):\n",
    "        # `sa_array`: selected path in tree [state0, action0, state1, action1, ..., stateK],\n",
    "\n",
    "        assert len(sa_array) % 2 == 1 and len(sa_array) >= 3\n",
    "        for si in range(0, len(sa_array) - 1, 2):\n",
    "            state, action = sa_array[si], sa_array[si + 1]\n",
    "\n",
    "            # reward = 1 iff player 0 wins\n",
    "            # if `state.player_idx` == 0 -> then it gets reward of 1\n",
    "            # if `state.player_idx` == 1 -> then it gets reward of -1\n",
    "            # if reward is -1, then all values are opposite\n",
    "\n",
    "            self.N_cnt_visits[state][action] += 1\n",
    "            self.W_sum_vals[state][action] += reward * (1 - 2 * state.player_idx)\n",
    "            self.Q_values[state][action] = self.W_sum_vals[state][action] / self.N_cnt_visits[state][action]\n",
    "\n",
    "    def clear(self):\n",
    "        self.N_cnt_visits.clear()\n",
    "        self.W_sum_vals.clear()\n",
    "        self.Q_values.clear()\n",
    "        self.P_action_probs.clear()\n",
    "        self.V_state_values.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate models and hold the best model so far\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_evaluate_games: int, n_max_steps: int, best_model=None) -> None:\n",
    "        self.n_evaluate_games = n_evaluate_games\n",
    "        self.n_max_steps = n_max_steps\n",
    "        self.best_model = best_model if best_model is not None else AlphaGoZeroModel()\n",
    "        self.best_agent = AlphaGoZeroAgent(self.best_model)\n",
    "\n",
    "    def clone_best_model(self):\n",
    "        new_model = AlphaGoZeroModel()\n",
    "        new_model.load_state_dict(copy.deepcopy(self.best_model.state_dict()))\n",
    "        return new_model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def battle_new_candidate(self, env, candidate_model: AlphaGoZeroModel):\n",
    "        \"\"\"\n",
    "        Take candidate model and play it against current best\n",
    "        Return true if the candidate model is better and become a new best model\n",
    "        \"\"\"\n",
    "        self.best_model.eval()\n",
    "        candidate_model.eval()\n",
    "\n",
    "        candidate_agent = AlphaGoZeroAgent(candidate_model)\n",
    "        results = battle(env, candidate_agent, self.best_agent, n_games=self.n_evaluate_games, n_max_steps=self.n_max_steps)\n",
    "\n",
    "        wins = results[0] + results[1]\n",
    "        losses = results[2] + results[3]\n",
    "\n",
    "        if wins >= 0.55 * self.n_evaluate_games:\n",
    "            print(f\"Candidate is a new champion: wins {wins}, losses {losses}, ties {results[4]}. Results={results}\")\n",
    "            self.best_model = candidate_model\n",
    "            self.best_agent = candidate_agent\n",
    "            return True\n",
    "\n",
    "        print(f\"Candidate defeated: wins {wins}, losses {losses}, ties {results[4]}. Results={results}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZero_Impl:\n",
    "    def __init__(self,\n",
    "        env,\n",
    "        agent_evaluator: AgentEvaluator,\n",
    "        mem_max_episodes: int,\n",
    "        n_iterations: int,\n",
    "        games_per_iteration: int,\n",
    "        simulations_cnt: int,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        batch_size: int,\n",
    "        root_dirichlet_alpha: float,\n",
    "        root_exploration_fraction: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        env: connect four compatible environment\n",
    "        agent_evaluator: to evaluate models and keep track of the current best model\n",
    "        mem_max_episodes: how many episodes keep in experience replay\n",
    "        n_iterations: how many iterations to do (one iteration consists of one improvement step and one agent evaluation step)\n",
    "        games_per_iteration: how many games best player plays against itself in each iteration\n",
    "        simulations_cnt: how many MCTS simulations run in each iteration\n",
    "        learning_rate: learning rate for model optimizer\n",
    "        weight_decay: weight decay for model training loss\n",
    "        batch_size: batch size for model training\n",
    "        root_dirichlet_alpha: noise power at each root state for selection\n",
    "        root_exploration_fraction: noise fraction added at each root state for selection\n",
    "        \"\"\"\n",
    "        self.root_env = env\n",
    "        self.agent_evaluator = agent_evaluator\n",
    "        self.n_iterations = n_iterations\n",
    "        self.games_per_iteration = games_per_iteration\n",
    "        self.simulations_cnt = simulations_cnt\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.root_dirichlet_alpha = root_dirichlet_alpha\n",
    "        self.root_exploration_fraction = root_exploration_fraction\n",
    "        self.tree = MCTS_Tree()\n",
    "        self.experience_replay = ExperienceReplay(max_episodes=mem_max_episodes)\n",
    "\n",
    "    def simulate(self, env, model: AlphaGoZeroModel, state, actions_mask):\n",
    "        while True:\n",
    "            action = model.get_rollout_action(state, actions_mask)\n",
    "            state, actions_mask, reward, done = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        return reward\n",
    "\n",
    "    def select(self, env):\n",
    "        tree = self.tree\n",
    "        state, actions_mask = env.last()\n",
    "        sa_array = [state]    # selected path [state0, action0, state1, action1, ..., stateK],\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        if state in tree.V_state_values:\n",
    "            noise = np.random.gamma(self.root_dirichlet_alpha, 1, len(actions_mask))\n",
    "            frac = self.root_exploration_fraction\n",
    "            tree.P_action_probs[state] = tree.P_action_probs[state] * (1 - frac) + noise * frac\n",
    "\n",
    "        while sa_array[-1] in tree.V_state_values:\n",
    "            action = tree.get_action(state)\n",
    "            state, actions_mask, reward, done = env.step(action)\n",
    "\n",
    "            sa_array.append(action)\n",
    "            sa_array.append(state)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Guarantees:\n",
    "        #    `sa_array[-1] not in tree.states_set`\n",
    "        #    `actions_mask, reward, done` tied to `sa_array[-1]`\n",
    "        #    `env` moves to state `sa_array[-1]`\n",
    "        return sa_array, actions_mask, reward, done\n",
    "\n",
    "    def expand(self, env, model: AlphaGoZeroModel, sa_array, actions_mask):\n",
    "        action = model.get_rollout_action(sa_array[-1], actions_mask)\n",
    "        state, actions_mask, reward, done = env.step(action)\n",
    "\n",
    "        sa_array.append(action)\n",
    "        sa_array.append(state)\n",
    "        return reward, done\n",
    "\n",
    "    def mcts_step(self, root_env, model: AlphaGoZeroModel):\n",
    "        tree = self.tree\n",
    "\n",
    "        for si in range(self.simulations_cnt):\n",
    "            env = root_env.copy()\n",
    "\n",
    "            sa_array, actions_mask, reward, done = self.select(env)\n",
    "\n",
    "            if not done:\n",
    "                tree.add_node(model, sa_array[-1], actions_mask)\n",
    "\n",
    "                reward, done = self.expand(env, model, sa_array, actions_mask)\n",
    "\n",
    "                if not done:\n",
    "                    reward = self.simulate(env, model, sa_array[-1], actions_mask)\n",
    "\n",
    "            tree.backpropagate(sa_array, reward)\n",
    "\n",
    "        state, actions_mask = root_env.last()\n",
    "        N_s_a = np.sqrt(tree.N_cnt_visits[state])\n",
    "        pi_probs = N_s_a / np.sum(N_s_a)\n",
    "        pi_probs[actions_mask == 0] = 0\n",
    "        pi_probs /= np.sum(pi_probs)\n",
    "        return np.random.choice(len(actions_mask), p=pi_probs)\n",
    "\n",
    "    def collect_tree_statistics_using_self_play(self, model: AlphaGoZeroModel):\n",
    "        experience_replay = self.experience_replay\n",
    "\n",
    "        root_env = self.root_env\n",
    "\n",
    "        for game_i in range(self.games_per_iteration):\n",
    "            state, mask = root_env.reset()\n",
    "            experience_replay.on_reset(state)\n",
    "            while True:\n",
    "                action = self.mcts_step(root_env, model)\n",
    "                state, mask, reward, done = root_env.step(action)\n",
    "                experience_replay.on_action(action, reward, done, state)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def go_train(self):\n",
    "        agent_evaluator = self.agent_evaluator\n",
    "        experience_replay = self.experience_replay\n",
    "        tree = self.tree\n",
    "\n",
    "        candidate_model = None\n",
    "        optimizer = None\n",
    "\n",
    "        for iter_i in range(self.n_iterations):\n",
    "            best_model = agent_evaluator.best_model\n",
    "            best_model.eval()\n",
    "            with torch.no_grad():\n",
    "                self.collect_tree_statistics_using_self_play(agent_evaluator.best_model)\n",
    "\n",
    "            if candidate_model is None:\n",
    "                candidate_model = agent_evaluator.clone_best_model()\n",
    "                optimizer = torch.optim.AdamW(candidate_model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "            candidate_model.train()\n",
    "            candidate_model.go_train(tree, experience_replay, optimizer, self.batch_size)\n",
    "            experience_replay.clear()\n",
    "\n",
    "            candidate_model.eval()\n",
    "            with torch.no_grad():\n",
    "                if agent_evaluator.battle_new_candidate(self.root_env, candidate_model):\n",
    "                    # Yay, new champion! Reset all things to start improving new best model\n",
    "                    candidate_model = None\n",
    "                    optimizer = None\n",
    "                    tree.clear()\n",
    "\n",
    "def train_alphagozero_connectfour(agent_evaluator=None):\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    if agent_evaluator is None:\n",
    "        agent_evaluator = AgentEvaluator(\n",
    "            n_evaluate_games=400,\n",
    "            n_max_steps=50,\n",
    "        )\n",
    "\n",
    "    impl = AlphaGoZero_Impl(\n",
    "        agent_evaluator=agent_evaluator,\n",
    "        env=env,\n",
    "        mem_max_episodes=100000,\n",
    "        n_iterations=20,\n",
    "        games_per_iteration=100,\n",
    "        simulations_cnt=20,\n",
    "        learning_rate=1e-2,\n",
    "        weight_decay=1e-5,\n",
    "        batch_size=16,\n",
    "        root_dirichlet_alpha=0.3,   # like in chess\n",
    "        root_exploration_fraction=0.25,\n",
    "    )\n",
    "    impl.go_train()\n",
    "\n",
    "    return {\n",
    "        'env': env,\n",
    "        'agent_evaluator': agent_evaluator,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate defeated: wins 0, losses 400, ties 0. Results=[0, 0, 200, 200, 0]\n",
      "Candidate is a new champion: wins 400, losses 0, ties 0. Results=[200, 200, 0, 0, 0]\n",
      "Candidate defeated: wins 0, losses 400, ties 0. Results=[0, 0, 200, 200, 0]\n",
      "Candidate is a new champion: wins 400, losses 0, ties 0. Results=[200, 200, 0, 0, 0]\n",
      "Candidate is a new champion: wins 400, losses 0, ties 0. Results=[200, 200, 0, 0, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[0, 200, 0, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[0, 200, 0, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[200, 0, 200, 0, 0]\n",
      "Candidate is a new champion: wins 400, losses 0, ties 0. Results=[200, 200, 0, 0, 0]\n",
      "Candidate is a new champion: wins 400, losses 0, ties 0. Results=[200, 200, 0, 0, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[0, 200, 0, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[200, 0, 200, 0, 0]\n",
      "Candidate defeated: wins 0, losses 400, ties 0. Results=[0, 0, 200, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[200, 0, 200, 0, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[0, 200, 0, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[0, 200, 0, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[200, 0, 200, 0, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[200, 0, 200, 0, 0]\n",
      "Candidate defeated: wins 0, losses 400, ties 0. Results=[0, 0, 200, 200, 0]\n",
      "Candidate defeated: wins 200, losses 200, ties 0. Results=[200, 0, 200, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# trained = train_alphagozero_connectfour()\n",
    "trained = train_alphagozero_connectfour(trained['agent_evaluator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 39, 461, 431, 0]\n"
     ]
    }
   ],
   "source": [
    "def show_trained(trained):\n",
    "    env = trained['env']\n",
    "    agent_evaluator = trained['agent_evaluator']\n",
    "    agent = agent_evaluator.best_agent\n",
    "\n",
    "    agent.model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(battle(env, RandomAgent(), agent, n_games=1000, n_max_steps=50))\n",
    "\n",
    "show_trained(trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...0...\n"
     ]
    }
   ],
   "source": [
    "def play_vs_ai(env, opponent, action):\n",
    "    state, action_mask, reward, done = env.step(action)\n",
    "\n",
    "    print(f\"Your move {action}: actions mask {list(action_mask)}, reward {reward}, done {done}.\")\n",
    "    env.render_ascii()\n",
    "\n",
    "    if not done:\n",
    "        action = opponent.get_action(state, action_mask)\n",
    "        state, action_mask, reward, done = env.step(action)\n",
    "        print()\n",
    "        print(f\"Opponent move {action}: actions mask {list(action_mask)}, reward {reward}, done {done}\")\n",
    "        env.render_ascii()\n",
    "\n",
    "test_opponent = AlphaGoZeroAgent(trained['agent_evaluator'].best_model)\n",
    "\n",
    "test_env = connect_four.ConnectFour()\n",
    "test_env.reset()\n",
    "test_env.step(test_opponent.get_action(test_env.last()[0], test_env.last()[1]))\n",
    "test_env.render_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your move 4: actions mask [1, 1, 1, 1, 1, 1, 1], reward -1, done True.\n",
      ".......\n",
      "...0...\n",
      "..001..\n",
      "..011..\n",
      "..101..\n",
      ".0101..\n"
     ]
    }
   ],
   "source": [
    "play_vs_ai(test_env, test_opponent, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Add symmetrical boards\n",
    "# Cpuct: supplementary materials p5\n",
    "# LR schedule: supplementary materials p7\n",
    "# Paper: tree is new in each mcts run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
