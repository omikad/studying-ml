{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Env based on: https://pettingzoo.farama.org/environments/classic/connect_four/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "\n",
    "import connect_four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 3 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 1 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 0 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 5 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 6 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 2 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "0 4 (0, False, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      "1 3 (-1, True, (6, 7, 2), array([1, 1, 1, 1, 1, 1, 1], dtype=int32))\n",
      ".......\n",
      "....0..\n",
      "....0.0\n",
      "101.0.0\n",
      "0111101\n",
      "0111001\n"
     ]
    }
   ],
   "source": [
    "def show():\n",
    "    # env = pettingzoo.classic.connect_four_v3.env(render_mode=\"human\")\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    state, mask = env.reset()\n",
    "\n",
    "    for step in range(50):\n",
    "        I = np.where(mask == 1)[0]\n",
    "        action = np.random.choice(I)\n",
    "\n",
    "        state, mask, reward, done = env.step(action)\n",
    "\n",
    "        print(step % 2, action, (reward, done, state.board.shape, mask))\n",
    "\n",
    "        if done:\n",
    "            env.render_ascii()\n",
    "            break\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random vs Random: wins 244, loses 255, draws 1\n",
      "Random vs AlwaysLeft: wins 108, loses 392, draws 0\n",
      "Random vs AlwaysRight: wins 99, loses 401, draws 0\n",
      "AlwaysLeft vs AlwaysRight: wins 250, loses 250, draws 0\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        action = np.random.choice(I)\n",
    "        return action\n",
    "\n",
    "\n",
    "class AlwaysLeftAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        return I[0]\n",
    "\n",
    "\n",
    "class AlwaysRightAgent:\n",
    "    def get_action(self, state: connect_four.HashableState, action_mask):\n",
    "        I = np.where(action_mask == 1)[0]\n",
    "        return I[-1]\n",
    "\n",
    "\n",
    "def battle(env, agent0, agent1, n_games, n_max_steps):\n",
    "    \"\"\"\n",
    "    Play `n_games` where each game is at most `n_max_steps` turns.\n",
    "    Return counter of game results as array `[agent 1 wins, agent 2 wins, draws]`\n",
    "    \"\"\"\n",
    "\n",
    "    results = [0, 0, 0]\n",
    "    agents = [agent0, agent1]\n",
    "\n",
    "    for game in range(n_games):\n",
    "        winner = 2\n",
    "        state, mask = env.reset()\n",
    "        who = game % 2  # Switch sides every other game\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "            action = agents[who].get_action(state, mask)\n",
    "\n",
    "            state, mask, reward, done = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                if reward != 0:\n",
    "                    winner = who\n",
    "                break\n",
    "            who = 1 - who\n",
    "\n",
    "        results[winner] += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def show_sample_agents_battle():\n",
    "    env = connect_four.ConnectFour()\n",
    "\n",
    "    for name0, agent0, name1, agent1 in [\n",
    "        (\"Random\", RandomAgent(), \"Random\", RandomAgent()),\n",
    "        (\"Random\", RandomAgent(), \"AlwaysLeft\", AlwaysLeftAgent()),\n",
    "        (\"Random\", RandomAgent(), \"AlwaysRight\", AlwaysRightAgent()),\n",
    "        (\"AlwaysLeft\", AlwaysLeftAgent(), \"AlwaysRight\", AlwaysRightAgent()),\n",
    "    ]:\n",
    "        battle_results = battle(env, agent0, agent1, n_games=500, n_max_steps=50)\n",
    "        print(f\"{name0} vs {name1}: wins {battle_results[0]}, loses {battle_results[1]}, draws {battle_results[2]}\")\n",
    "\n",
    "show_sample_agents_battle()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo Zero plays Connect Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaGoZeroModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement AlphaGo Zero model with two heads: for action probabilities and state value\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_common = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),   # (6, 7, 2) -> 84\n",
    "            torch.nn.Linear(84, 32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.model_action_probs = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 7),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.model_state_value = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, board):\n",
    "        B = board.shape[0]\n",
    "        assert board.shape == (B, 6, 7, 2)\n",
    "\n",
    "        common = self.model_common(board)\n",
    "        action_probs = self.model_action_probs(common)\n",
    "        state_value = self.model_state_value(common)\n",
    "\n",
    "        # action_probs.shape == (B, 7)\n",
    "        # state_value.shape == (B, 1)\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "    def get_p_v_single_state(self, state: connect_four.HashableState, need_p: bool, need_v: bool):\n",
    "        board, player_idx = state.board, state.player_idx\n",
    "        assert board.shape == (6, 7, 2)\n",
    "\n",
    "        if player_idx == 1:\n",
    "            # Make current player to have coins at board[:, :, 0], and opponent at board[:, :, 1]\n",
    "            board = np.flip(board, axis=2).copy()\n",
    "\n",
    "        board = torch.from_numpy(board.reshape((1, 6, 7, 2))).float()\n",
    "        common = self.model_common(board)\n",
    "\n",
    "        action_probs, state_value = None, None\n",
    "\n",
    "        if need_p:\n",
    "            action_probs = self.model_action_probs(common)\n",
    "            action_probs = action_probs.detach().cpu().numpy()[0, :]\n",
    "\n",
    "        if need_v:\n",
    "            state_value = self.model_state_value(common)\n",
    "            state_value = state_value.detach().cpu().numpy()[0, 0]\n",
    "\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "    def get_rollout_action(self, state: connect_four.HashableState, actions_mask):\n",
    "        assert actions_mask.shape == (7,)\n",
    "\n",
    "        action_probs, _ = self.get_p_v_single_state(state, need_p=True, need_v=False)\n",
    "        action_probs[actions_mask == 0] = -np.inf\n",
    "\n",
    "        return np.argmax(action_probs)\n",
    "\n",
    "    def go_train(self, tree, optimizer, batch_size):\n",
    "        self.train()\n",
    "\n",
    "        all_states = list(tree.V_state_values.keys())\n",
    "        np.random.shuffle(all_states)\n",
    "        for bi in range(batch_size, len(all_states) + 1, batch_size):\n",
    "            states = all_states[bi - batch_size : bi]\n",
    "\n",
    "            board = [state.board for state in states]\n",
    "            board = torch.as_tensor(board).float()\n",
    "            # print(f\"[go_train] board {board.shape} {board.dtype}\")   # (B, 7, 8, 2)\n",
    "\n",
    "            action_probs, state_value = self.forward(board)\n",
    "            # print(f\"[go_train] action_probs {action_probs.shape} {action_probs.dtype}\")  # (B, 7)\n",
    "            # print(f\"[go_train] state_value {state_value.shape} {state_value.dtype}\")  # (B, 1)\n",
    "\n",
    "            actual_probs = np.array([ tree.N_cnt_visits[state] for state in states ])\n",
    "            # print(f\"[go_train] actual_probs {actual_probs.shape} {actual_probs.dtype}\")  # (B, 7)\n",
    "            actual_probs /= np.sum(actual_probs, axis=1).reshape((batch_size, 1))\n",
    "\n",
    "            actual_values = np.array([ np.max(tree.Q_values[state]) for state in states ])\n",
    "            # print(f\"[go_train] actual_values {actual_values.shape} {actual_values.dtype}\")  # (B,)\n",
    "\n",
    "            actual_probs = torch.as_tensor(actual_probs)\n",
    "            actual_values = torch.as_tensor(actual_values).view((batch_size, 1))\n",
    "\n",
    "            states_loss = torch.sum(torch.abs((actual_values - state_value)**2))\n",
    "\n",
    "            action_loss = torch.sum(actual_probs * torch.log(action_probs))\n",
    "\n",
    "            loss = states_loss + action_loss\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "class AlphaGoZeroAgent:\n",
    "    \"\"\"\n",
    "    Trained agent which can play Connect Four game using `AlphaGoZeroModel` model\n",
    "    \"\"\"\n",
    "    def __init__(self, model: AlphaGoZeroModel) -> None:\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, state: connect_four.HashableState, actions_mask):\n",
    "        self.model.eval()\n",
    "        return self.model.get_rollout_action(state, actions_mask)\n",
    "\n",
    "\n",
    "class MCTS_Tree:\n",
    "    \"\"\"\n",
    "    Tree structure which keeps various statistics needed for MCTS\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # Structure of all dicts: { state -> [ action -> value ] }\n",
    "        self.N_cnt_visits = dict()\n",
    "        self.W_sum_vals = dict()\n",
    "        self.Q_values = dict()\n",
    "        self.P_action_probs = dict()\n",
    "\n",
    "        self.V_state_values = dict()  # { state -> predicted state value v(s) }\n",
    "\n",
    "    def get_action(self, env, state: connect_four.HashableState, actions_mask):\n",
    "        N_s_a = self.N_cnt_visits[state]\n",
    "        Q_s_a = self.Q_values[state]\n",
    "        P_s_a = self.P_action_probs[state]\n",
    "        U_s_a = 4 * P_s_a * np.sum(N_s_a) / (1 + N_s_a)\n",
    "        action = np.argmax(Q_s_a + U_s_a)\n",
    "        return action\n",
    "\n",
    "    def add_node(self, model: AlphaGoZeroModel, state, actions_mask):\n",
    "        action_probs, state_value = model.get_p_v_single_state(state, need_p=True, need_v=True)\n",
    "        action_probs[actions_mask == 0] = 0\n",
    "        action_probs /= np.sum(action_probs)\n",
    "\n",
    "        actions_len = len(actions_mask)\n",
    "        self.N_cnt_visits[state] = np.zeros(actions_len)\n",
    "        self.W_sum_vals[state] = np.zeros(actions_len)\n",
    "        self.Q_values[state] = np.zeros(actions_len)\n",
    "        self.Q_values[state][actions_mask == 0] = -np.inf\n",
    "        self.P_action_probs[state] = action_probs\n",
    "        self.V_state_values[state] = state_value\n",
    "\n",
    "    def backpropagate(self, sa_array, reward):\n",
    "        # `sa_array`: selected path in tree [state0, action0, state1, action1, ..., stateK],\n",
    "\n",
    "        assert len(sa_array) % 2 == 1 and len(sa_array) >= 3\n",
    "        for si in range(0, len(sa_array) - 1, 2):\n",
    "            state, action = sa_array[si], sa_array[si + 1]\n",
    "\n",
    "            # reward = 1 iff player 0 wins\n",
    "            # if `state.player_idx` == 0 -> then it gets reward of 1\n",
    "            # if `state.player_idx` == 1 -> then it gets reward of -1\n",
    "            # if reward is -1, then all values are opposite\n",
    "\n",
    "            self.N_cnt_visits[state][action] += 1\n",
    "            self.W_sum_vals[state][action] += reward * (1 - 2 * state.player_idx)\n",
    "            self.Q_values[state][action] = self.W_sum_vals[state][action] / self.N_cnt_visits[state][action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4840/2248979221.py:87: RuntimeWarning: invalid value encountered in divide\n",
      "  actual_probs /= np.sum(actual_probs, axis=1).reshape((batch_size, 1))\n"
     ]
    }
   ],
   "source": [
    "class AlphaGoZero_Impl:\n",
    "    def __init__(self,\n",
    "        env,\n",
    "        games_per_iteration,\n",
    "        simulations_cnt,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        batch_size\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        games_per_iteration: how many games best player plays against itself in each iteration\n",
    "        simulations_cnt: how many MCTS simulations run in each iteration\n",
    "        learning_rate: learning rate for model optimizer\n",
    "        weight_decay: weight decay for model training loss\n",
    "        batch_size: batch size for model training\n",
    "        \"\"\"\n",
    "        self.root_env = env\n",
    "        self.games_per_iteration = games_per_iteration\n",
    "        self.simulations_cnt = simulations_cnt\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def simulate(self, env, model: AlphaGoZeroModel, state, actions_mask):\n",
    "        model.eval()\n",
    "\n",
    "        while True:\n",
    "            action = model.get_rollout_action(state, actions_mask)\n",
    "            state, actions_mask, reward, done = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        return reward\n",
    "\n",
    "    def select(self, env, tree: MCTS_Tree):\n",
    "        state, actions_mask = env.last()\n",
    "        sa_array = [state]    # selected path [state0, action0, state1, action1, ..., stateK],\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        while sa_array[-1] in tree.V_state_values:\n",
    "            action = tree.get_action(env, state, actions_mask)\n",
    "            state, actions_mask, reward, done = env.step(action)\n",
    "\n",
    "            sa_array.append(action)\n",
    "            sa_array.append(state)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Guarantees:\n",
    "        #    `sa_array[-1] not in tree.states_set`\n",
    "        #    `actions_mask, reward, done` tied to `sa_array[-1]`\n",
    "        #    `env` moves to state `sa_array[-1]`\n",
    "        return sa_array, actions_mask, reward, done\n",
    "\n",
    "    def expand(self, env, model: AlphaGoZeroModel, sa_array, actions_mask):\n",
    "        action = model.get_rollout_action(sa_array[-1], actions_mask)\n",
    "        state, actions_mask, reward, done = env.step(action)\n",
    "\n",
    "        sa_array.append(action)\n",
    "        sa_array.append(state)\n",
    "        return reward, done\n",
    "\n",
    "    def mcts_step(self, root_env, model: AlphaGoZeroModel, tree: MCTS_Tree):\n",
    "        for si in range(self.simulations_cnt):\n",
    "            env = root_env.copy()\n",
    "\n",
    "            sa_array, actions_mask, reward, done = self.select(env, tree)\n",
    "\n",
    "            tree.add_node(model, sa_array[-1], actions_mask)\n",
    "\n",
    "            if not done:\n",
    "                reward, done = self.expand(env, model, sa_array, actions_mask)\n",
    "\n",
    "                if not done:\n",
    "                    reward = self.simulate(env, model, sa_array[-1], actions_mask)\n",
    "\n",
    "            tree.backpropagate(sa_array, reward)\n",
    "\n",
    "        state, actions_mask = root_env.last()\n",
    "        N_s_a = tree.N_cnt_visits[state]\n",
    "        pi_probs = N_s_a / np.sum(N_s_a)\n",
    "        pi_probs[actions_mask == 0] = 0\n",
    "        pi_probs /= np.sum(pi_probs)\n",
    "        return np.random.choice(len(actions_mask), p=pi_probs)\n",
    "\n",
    "    def improve_policy_using_self_play(self, model: AlphaGoZeroModel, optimizer):\n",
    "        root_env = self.root_env\n",
    "        tree = MCTS_Tree()\n",
    "\n",
    "        for game_i in range(self.games_per_iteration):\n",
    "            root_env.reset()\n",
    "            while True:\n",
    "                action = self.mcts_step(root_env, model, tree)\n",
    "\n",
    "                state, mask, reward, done = root_env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        model.go_train(tree, optimizer, self.batch_size)\n",
    "\n",
    "    def go_train(self):\n",
    "        model = AlphaGoZeroModel()\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        self.improve_policy_using_self_play(model, optimizer)\n",
    "        return model\n",
    "\n",
    "\n",
    "def train_alphagozero_connectfour():\n",
    "    env = connect_four.ConnectFour()\n",
    "    impl = AlphaGoZero_Impl(\n",
    "        env=env,\n",
    "        games_per_iteration=10,\n",
    "        simulations_cnt=100,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    model = impl.go_train()\n",
    "    return env, model\n",
    "\n",
    "trained = train_alphagozero_connectfour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201, 799, 0]\n"
     ]
    }
   ],
   "source": [
    "def show_trained(trained):\n",
    "    env, model = trained\n",
    "    print(battle(env, RandomAgent(), AlphaGoZeroAgent(model), n_games=1000, n_max_steps=50))\n",
    "\n",
    "show_trained(trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
